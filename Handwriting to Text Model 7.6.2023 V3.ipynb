{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "conda activate TFgpu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div align=\"center\">\n",
    "  <img src=\"DisplayIMG/Cartoon.png\" alt=\"Image Description\" />\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as s\n",
    "from PIL import Image\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout, BatchNormalization\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import ReduceLROnPlateau, EarlyStopping, ModelCheckpoint\n",
    "from keras_tuner import BayesianOptimization, HyperParameters\n",
    "import cv2\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras.backend as K\n",
    "import seaborn as sns "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_cores = os.cpu_count()  # Get number of CPU cores\n",
    "num_cores_to_use = num_cores // 2  # Use half of the cores\n",
    "\n",
    "num_cores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_cores_to_use"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "current_directory = os.getcwd()\n",
    "\n",
    "folder_name = \"img\"\n",
    "\n",
    "folder_path = os.path.join(current_directory, folder_name)\n",
    "\n",
    "csv_file_path = os.path.join(current_directory, \"labels\", \"english-clean.csv\")\n",
    "\n",
    "df = pd.read_csv(csv_file_path)\n",
    "\n",
    "# Create a dictionary where keys are filenames and values are labels\n",
    "labels_dict = df.set_index('image')['label'].to_dict()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Helper Function for Loading Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_images_from_folder(folder, labels_dict):\n",
    "    images = []\n",
    "    labels = []\n",
    "    for file_path in glob.glob(os.path.join(folder, \"*.png\")):\n",
    "        img = Image.open(file_path)\n",
    "        if img is not None:\n",
    "            img = img.convert('L')  # Convert image to grayscale\n",
    "            img = img.resize((64, 64))  # Resize the image to your desired size\n",
    "            np_img = np.array(img)\n",
    "            images.append(np_img)\n",
    "            filename = os.path.basename(file_path)\n",
    "            label = labels_dict[filename]  # Get the label from the filename\n",
    "            labels.append(label)\n",
    "    return images, labels\n",
    "\n",
    "\n",
    "images, labels = load_images_from_folder(folder_path, labels_dict)\n",
    "\n",
    "le = preprocessing.LabelEncoder()\n",
    "numerical_labels = le.fit_transform(labels)  # Convert labels to integers\n",
    "\n",
    "images = np.array(images)  # Convert list of arrays to a single array\n",
    "mean = np.mean(images)\n",
    "std = np.std(images)\n",
    "images = (images - mean) / std  # Normalize pixel values\n",
    "images = images.reshape(-1, 64, 64, 1)  # Reshape array for CNN\n",
    "\n",
    "encoded_labels = to_categorical(numerical_labels)  # One-hot encode labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Split and Fit Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div align=\"center\">\n",
    "  <img src=\"DisplayIMG/Split & Fit.png\" alt=\"Image Description\" />\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(images, encoded_labels, test_size=0.2, random_state=42,\n",
    "                                                    shuffle=True)\n",
    "\n",
    "# Data augmentation for training data\n",
    "datagen = ImageDataGenerator(\n",
    "    rotation_range=35,  # Rotate images randomly by **% degrees\n",
    "    width_shift_range=0.1,  # Shift images horizontally by **% of the width\n",
    "    height_shift_range=0.1,  # Shift images vertically by **% of the height\n",
    "    shear_range=0.0,  # Apply shear transformation with a shear intensity of **\n",
    "    zoom_range=0.1,  # Apply zoom transformation with a zoom range of **\n",
    "    horizontal_flip=False,  # Do not Flip images horizontally\n",
    "    vertical_flip=False  # Do not flip images vertically\n",
    ")\n",
    "\n",
    "datagen.fit(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div align=\"center\">\n",
    "  <img src=\"DisplayIMG/i_model.png\" alt=\"Image Description\" />\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_decay = ReduceLROnPlateau(factor=0.1, patience=10)\n",
    "\n",
    "\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "# First block of convolutions\n",
    "model.add(Conv2D(32, (3, 3), activation='relu', padding='same', input_shape=(64, 64, 1)))  #Add Notes here******\n",
    "model.add(BatchNormalization())                                                            #Add Notes here******\n",
    "model.add(Conv2D(32, (3, 3), activation='relu', padding='same'))                           #Add Notes here******\n",
    "model.add(BatchNormalization())                                                            \n",
    "model.add(MaxPooling2D((2, 2)))                                                            #Add Notes here******\n",
    "model.add(Dropout(0.1))                                                                    #Add Notes here******\n",
    "\n",
    "# Second block of convolutions\n",
    "model.add(Conv2D(64, (3, 3), activation='relu', padding='same'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Conv2D(64, (3, 3), activation='relu', padding='same'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPooling2D((2, 2)))\n",
    "model.add(Dropout(0.1))\n",
    "\n",
    "# Fully connected layers\n",
    "model.add(Flatten())                                                                       #Add Notes here******\n",
    "model.add(Dense(512, activation='relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dropout(0.1))\n",
    "model.add(Dense(len(le.classes_), activation='softmax'))                                   #Add Notes here******\n",
    "\n",
    "# Compile model\n",
    "model.compile(optimizer=Adam(learning_rate=0.01),                                          #Add Notes here******\n",
    "              loss='categorical_crossentropy',                                             #Add Notes here******\n",
    "              metrics=['accuracy'])                                                        #Add Notes here******\n",
    "\n",
    "\n",
    "history = model.fit(datagen.flow(X_train, y_train),\n",
    "                    steps_per_epoch=len(X_train) // 32,\n",
    "                    epochs=200,                                                            # Number of Runs in the Model\n",
    "                    validation_data=(X_test, y_test),\n",
    "                    callbacks=[lr_decay])                                                  #Add Notes here******"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div align=\"center\">\n",
    "  <img src=\"DisplayIMG/Data_Lung.png\" alt=\"Image Description\" />\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'history' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m plt\u001b[38;5;241m.\u001b[39msubplot(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m----> 2\u001b[0m plt\u001b[38;5;241m.\u001b[39mplot(\u001b[43mhistory\u001b[49m\u001b[38;5;241m.\u001b[39mhistory[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124maccuracy\u001b[39m\u001b[38;5;124m'\u001b[39m], label\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTrain Accuracy\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      3\u001b[0m plt\u001b[38;5;241m.\u001b[39mplot(history\u001b[38;5;241m.\u001b[39mhistory[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mval_accuracy\u001b[39m\u001b[38;5;124m'\u001b[39m], label\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mValidation Accuracy\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      4\u001b[0m plt\u001b[38;5;241m.\u001b[39mxlabel(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mEpochs\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'history' is not defined"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(history.history['accuracy'], label='Train Accuracy')\n",
    "plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(history.history['loss'], label='Training Loss')\n",
    "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "[7]\n",
    "# Make predictions on the test set\n",
    "y_pred = model.predict(X_test)\n",
    "y_pred_classes = np.argmax(y_pred, axis=1)\n",
    "y_true_classes = np.argmax(y_test, axis=1)\n",
    "\n",
    "# Print a classification report\n",
    "print(classification_report(y_true_classes, y_pred_classes, target_names=le.classes_, zero_division=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'history' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Plot training & validation accuracy values\u001b[39;00m\n\u001b[0;32m      2\u001b[0m plt\u001b[38;5;241m.\u001b[39mfigure(figsize\u001b[38;5;241m=\u001b[39m[\u001b[38;5;241m12\u001b[39m,\u001b[38;5;241m8\u001b[39m])\n\u001b[1;32m----> 3\u001b[0m plt\u001b[38;5;241m.\u001b[39mplot(\u001b[43mhistory\u001b[49m\u001b[38;5;241m.\u001b[39mhistory[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124maccuracy\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m      4\u001b[0m plt\u001b[38;5;241m.\u001b[39mplot(history\u001b[38;5;241m.\u001b[39mhistory[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mval_accuracy\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m      5\u001b[0m plt\u001b[38;5;241m.\u001b[39mtitle(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mModel accuracy\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'history' is not defined"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1200x800 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot training & validation accuracy values\n",
    "plt.figure(figsize=[12,8])\n",
    "plt.plot(history.history['accuracy'])\n",
    "plt.plot(history.history['val_accuracy'])\n",
    "plt.title('Model accuracy')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Test'], loc='upper left')\n",
    "plt.show()\n",
    "\n",
    "# Plot training & validation loss values\n",
    "plt.figure(figsize=[12,8])\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('Model loss')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Test'], loc='upper left')\n",
    "plt.show()\n",
    "\n",
    "# Evaluate the model on the test data using `evaluate`\n",
    "print(\"Evaluate on test data\")\n",
    "results = model.evaluate(X_test, y_test, batch_size=128)\n",
    "print(\"test loss, test accuracy:\", results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# #Save Model For Furture Use\n",
    "# model.save('EpochTenThousand')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pickle\n",
    "\n",
    "# # Assume you have a history object from model.fit\n",
    "# # history = model.fit(....)\n",
    "\n",
    "# # Save it under some name\n",
    "# with open('trainHistoryDict', 'wb') as file_pi:\n",
    "#     pickle.dump(history.history, file_pi)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open('trainHistoryDict', 'rb') as file_pi:\n",
    "#     loaded_history = pickle.load(file_pi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "86/86 [==============================] - 1s 11ms/step - loss: 0.1502 - accuracy: 0.9417\n",
      "Train Loss: 0.15020376443862915\n",
      "Train Accuracy: 0.9417155385017395\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model on the training set\n",
    "train_loss, train_accuracy = model.evaluate(X_train, y_train)\n",
    "print(\"Train Loss:\", train_loss)\n",
    "print(\"Train Accuracy:\", train_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/22 [==============================] - 0s 10ms/step - loss: 0.4190 - accuracy: 0.8490\n",
      "Test Loss: 0.41899049282073975\n",
      "Test Accuracy: 0.8489736318588257\n",
      "22/22 [==============================] - 0s 9ms/step\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.75      0.43      0.55        14\n",
      "           1       0.70      0.64      0.67        11\n",
      "           2       1.00      1.00      1.00        10\n",
      "           3       0.95      1.00      0.98        20\n",
      "           4       0.93      1.00      0.97        14\n",
      "           5       0.93      0.88      0.90        16\n",
      "           6       0.90      1.00      0.95         9\n",
      "           7       1.00      1.00      1.00        15\n",
      "           8       0.79      0.92      0.85        12\n",
      "           9       0.92      1.00      0.96        11\n",
      "          10       1.00      1.00      1.00        16\n",
      "          11       0.82      1.00      0.90         9\n",
      "          12       0.83      0.56      0.67         9\n",
      "          13       0.83      1.00      0.91        10\n",
      "          14       1.00      1.00      1.00        15\n",
      "          15       1.00      0.93      0.96        14\n",
      "          16       1.00      1.00      1.00        10\n",
      "          17       1.00      0.91      0.95        11\n",
      "          18       1.00      0.50      0.67        16\n",
      "          19       1.00      1.00      1.00         5\n",
      "          20       0.80      0.67      0.73        12\n",
      "          21       0.89      0.89      0.89         9\n",
      "          22       1.00      0.89      0.94         9\n",
      "          23       1.00      1.00      1.00         7\n",
      "          24       0.54      0.78      0.64         9\n",
      "          25       0.80      1.00      0.89         8\n",
      "          26       1.00      1.00      1.00        13\n",
      "          27       1.00      0.88      0.93         8\n",
      "          28       0.75      0.60      0.67        10\n",
      "          29       1.00      1.00      1.00        11\n",
      "          30       0.83      0.91      0.87        11\n",
      "          31       0.78      0.64      0.70        11\n",
      "          32       0.75      0.50      0.60        12\n",
      "          33       0.90      0.82      0.86        11\n",
      "          34       0.91      1.00      0.95        10\n",
      "          35       0.91      0.91      0.91        11\n",
      "          36       1.00      0.77      0.87        13\n",
      "          37       1.00      0.83      0.91         6\n",
      "          38       0.50      0.83      0.62         6\n",
      "          39       0.88      1.00      0.93         7\n",
      "          40       0.85      0.92      0.88        12\n",
      "          41       0.86      0.75      0.80         8\n",
      "          42       0.79      1.00      0.88        11\n",
      "          43       1.00      0.78      0.88         9\n",
      "          44       1.00      0.92      0.96        13\n",
      "          45       1.00      1.00      1.00         7\n",
      "          46       0.69      0.90      0.78        10\n",
      "          47       0.53      0.91      0.67        11\n",
      "          48       0.83      0.91      0.87        11\n",
      "          49       0.85      0.92      0.88        12\n",
      "          50       0.64      0.69      0.67        13\n",
      "          51       1.00      0.71      0.83         7\n",
      "          52       1.00      0.64      0.78        11\n",
      "          53       0.90      1.00      0.95         9\n",
      "          54       0.71      0.67      0.69        18\n",
      "          55       0.90      0.75      0.82        12\n",
      "          56       0.83      0.83      0.83        12\n",
      "          57       0.55      0.75      0.63         8\n",
      "          58       0.64      0.82      0.72        11\n",
      "          59       0.73      0.69      0.71        16\n",
      "          60       0.90      0.90      0.90        10\n",
      "          61       0.64      0.90      0.75        10\n",
      "\n",
      "    accuracy                           0.85       682\n",
      "   macro avg       0.86      0.85      0.85       682\n",
      "weighted avg       0.86      0.85      0.85       682\n",
      "\n",
      "Confusion Matrix:\n",
      "[[ 6  0  0 ...  0  0  0]\n",
      " [ 0  7  0 ...  0  0  1]\n",
      " [ 0  0 10 ...  0  0  0]\n",
      " ...\n",
      " [ 0  0  0 ... 11  0  2]\n",
      " [ 0  0  0 ...  0  9  0]\n",
      " [ 0  0  0 ...  0  0  9]]\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model on the test set\n",
    "loss, accuracy = model.evaluate(X_test, y_test)\n",
    "print(\"Test Loss:\", loss)\n",
    "print(\"Test Accuracy:\", accuracy)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = model.predict(X_test)\n",
    "y_pred_labels = np.argmax(y_pred, axis=1)\n",
    "y_true_labels = np.argmax(y_test, axis=1)\n",
    "\n",
    "# Print classification report\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_true_labels, y_pred_labels, zero_division=1))\n",
    "\n",
    "# Print confusion matrix\n",
    "from sklearn.metrics import confusion_matrix\n",
    "confusion_mtx = confusion_matrix(y_true_labels, y_pred_labels)\n",
    "print(\"Confusion Matrix:\")\n",
    "print(confusion_mtx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Make predictions on the test set\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m y_pred \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241m.\u001b[39mpredict(X_test)\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# Convert probabilities to labels\u001b[39;00m\n\u001b[0;32m      5\u001b[0m y_pred_labels \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39margmax(y_pred, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Convert probabilities to labels\n",
    "y_pred_labels = np.argmax(y_pred, axis=1)\n",
    "\n",
    "# Also convert the one-hot encoded labels back to label encoding\n",
    "y_true_labels = np.argmax(y_test, axis=1)\n",
    "\n",
    "# Convert numerical labels back to original labels\n",
    "y_pred_labels = le.inverse_transform(y_pred_labels)\n",
    "y_true_labels = le.inverse_transform(y_true_labels)\n",
    "\n",
    "# Calculate confusion matrix\n",
    "confusion_mtx = confusion_matrix(y_true_labels, y_pred_labels)\n",
    "\n",
    "# Create a list of unique labels\n",
    "labels = list(le.classes_)\n",
    "\n",
    "# Set the font scale (this will affect heatmap annotation size)\n",
    "sns.set(font_scale=1.5)\n",
    "\n",
    "# Adjust size of labels, title using rcParams\n",
    "plt.rcParams['xtick.labelsize']=15\n",
    "plt.rcParams['ytick.labelsize']=15\n",
    "\n",
    "# Visualize confusion matrix using seaborn's heatmap\n",
    "plt.figure(figsize=(25, 20))\n",
    "sns.heatmap(confusion_mtx, annot=True, fmt='d', cmap='Blues', xticklabels=labels, yticklabels=labels)\n",
    "plt.xlabel('Predicted Label', fontsize=20)\n",
    "plt.ylabel('True Label', fontsize=20)\n",
    "plt.title('Confusion Matrix', fontsize=25)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "best_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Class Activation Maps (CAM):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow.keras.backend as K\n",
    "\n",
    "def visualize_cam(model, img):\n",
    "    # Extract the last convolutional layer\n",
    "    last_conv_layer = model.get_layer('conv2d_4')\n",
    "    \n",
    "    # Define a new model that outputs the last conv layer and the model's predicted class probabilities\n",
    "    cam_model = tf.keras.Model(model.input, (last_conv_layer.output, model.output))\n",
    "    \n",
    "    # Calculate the gradients of the predicted class with respect to the output feature map of the last conv layer\n",
    "    with tf.GradientTape() as tape:\n",
    "        conv_outputs, predictions = cam_model(img)\n",
    "        predicted_class = tf.argmax(predictions[0])\n",
    "        gradient = tape.gradient(predictions[:, predicted_class], conv_outputs)\n",
    "    \n",
    "    # Compute the weights using global average pooling on the gradients\n",
    "    weights = tf.reduce_mean(gradient[0], axis=(0, 1))\n",
    "    \n",
    "    # Get the feature map values and resize them to match the input image size\n",
    "    feature_map = conv_outputs[0]\n",
    "    cam = np.dot(feature_map, weights)\n",
    "    cam = cv2.resize(cam, (img.shape[2], img.shape[1]))\n",
    "    cam = np.maximum(cam, 0)\n",
    "    cam = cam / np.max(cam)\n",
    "    \n",
    "    return cam\n",
    "\n",
    "\n",
    "# Choose an image from the test set\n",
    "test_image = X_test[4]\n",
    "\n",
    "# Reshape the image to match the input shape of the model\n",
    "test_image = np.reshape(test_image, (1, 64, 64, 1))\n",
    "\n",
    "# Generate the CAM for the chosen image\n",
    "cam = visualize_cam(model, test_image)\n",
    "\n",
    "# Plot the original image and the CAM\n",
    "plt.figure(figsize=(8, 4))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.imshow(test_image[0, :, :, 0], cmap='gray')\n",
    "plt.title('Original Image')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.imshow(test_image[0, :, :, 0], cmap='gray')\n",
    "plt.imshow(cam, cmap='jet', alpha=0.5)\n",
    "plt.title('Class Activation Map')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def visualize_cam(model, img):\n",
    "    # Extract the last convolutional layer\n",
    "    last_conv_layer = model.get_layer('conv2d_3')\n",
    "    \n",
    "    # Define a new model that outputs the last conv layer and the model's predicted class probabilities\n",
    "    cam_model = tf.keras.Model(model.input, (last_conv_layer.output, model.output))\n",
    "    \n",
    "    # Calculate the gradients of the predicted class with respect to the output feature map of the last conv layer\n",
    "    with tf.GradientTape() as tape:\n",
    "        conv_outputs, predictions = cam_model(img)\n",
    "        predicted_class = tf.argmax(predictions[0])\n",
    "        gradient = tape.gradient(predictions[:, predicted_class], conv_outputs)\n",
    "    \n",
    "    # Compute the weights using global average pooling on the gradients\n",
    "    weights = tf.reduce_mean(gradient[0], axis=(0, 1))\n",
    "    \n",
    "    # Get the feature map values and resize them to match the input image size\n",
    "    feature_map = conv_outputs[0]\n",
    "    cam = np.dot(feature_map, weights)\n",
    "    cam = cv2.resize(cam, (img.shape[2], img.shape[1]))\n",
    "    cam = np.maximum(cam, 0)\n",
    "    cam = cam / np.max(cam)\n",
    "    \n",
    "    return cam\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "cam = visualize_cam(model, test_image)\n",
    "\n",
    "# Plot the original image and the CAM\n",
    "plt.figure(figsize=(8, 4))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.imshow(test_image[0, :, :, 0], cmap='gray')\n",
    "plt.title('Original Image')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.imshow(test_image[0, :, :, 0], cmap='gray')\n",
    "plt.imshow(cam, cmap='jet', alpha=0.5)\n",
    "plt.title('Class Activation Map')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_cam(model, img):\n",
    "    # Extract the last convolutional layer\n",
    "    last_conv_layer = model.get_layer('conv2d_2')\n",
    "    \n",
    "    # Define a new model that outputs the last conv layer and the model's predicted class probabilities\n",
    "    cam_model = tf.keras.Model(model.input, (last_conv_layer.output, model.output))\n",
    "    \n",
    "    # Reshape the input image to match the expected input shape of the model\n",
    "    img = np.reshape(img, (1, 64, 64, 1))\n",
    "    \n",
    "    # Calculate the gradients of the predicted class with respect to the output feature map of the last conv layer\n",
    "    with tf.GradientTape() as tape:\n",
    "        conv_outputs, predictions = cam_model(img)\n",
    "        predicted_class = tf.argmax(predictions[0])\n",
    "        gradient = tape.gradient(predictions[:, predicted_class], conv_outputs)\n",
    "    \n",
    "    # Compute the weights using global average pooling on the gradients\n",
    "    weights = tf.reduce_mean(gradient[0], axis=(0, 1))\n",
    "    \n",
    "    # Get the feature map values and resize them to match the input image size\n",
    "    feature_map = conv_outputs[0]\n",
    "    cam = np.dot(feature_map, weights)\n",
    "    cam = cv2.resize(cam, (img.shape[2], img.shape[1]))\n",
    "    cam = np.maximum(cam, 0)\n",
    "    cam = cam / np.max(cam)\n",
    "    \n",
    "    return cam\n",
    "\n",
    "cam = visualize_cam(model, test_image)\n",
    "\n",
    "# Plot the original image and the CAM\n",
    "plt.figure(figsize=(8, 4))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.imshow(test_image[0, :, :, 0], cmap='gray')\n",
    "plt.title('Original Image')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.imshow(test_image[0, :, :, 0], cmap='gray')\n",
    "plt.imshow(cam, cmap='jet', alpha=0.5)\n",
    "plt.title('Class Activation Map')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Class Activation Maps (CAM) of First Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_cam(model, img):\n",
    "    # Extract the last convolutional layer\n",
    "    last_conv_layer = model.get_layer('conv2d_2')\n",
    "    \n",
    "    # Define a new model that outputs the last conv layer and the model's predicted class probabilities\n",
    "    cam_model = tf.keras.Model(model.input, (last_conv_layer.output, model.output))\n",
    "    \n",
    "    # Reshape the input image to match the expected input shape of the model\n",
    "    img = np.reshape(img, (-1, 64, 64, 1))\n",
    "    \n",
    "    # Calculate the gradients of the predicted class with respect to the output feature map of the last conv layer\n",
    "    with tf.GradientTape() as tape:\n",
    "        conv_outputs, predictions = cam_model(img)\n",
    "        predicted_class = tf.argmax(predictions[0])\n",
    "        gradient = tape.gradient(predictions[:, predicted_class], conv_outputs)\n",
    "    \n",
    "    # Compute the weights using global average pooling on the gradients\n",
    "    weights = tf.reduce_mean(gradient[0], axis=(0, 1))\n",
    "    \n",
    "    # Get the feature map values and resize them to match the input image size\n",
    "    feature_map = conv_outputs[0]\n",
    "    cam = np.dot(feature_map, weights)\n",
    "    cam = cv2.resize(cam, (img.shape[2], img.shape[1]))\n",
    "    cam = np.maximum(cam, 0)\n",
    "    cam = cam / np.max(cam)\n",
    "    \n",
    "    return cam\n",
    "\n",
    "\n",
    "\n",
    "# Reshape the image to match the expected input shape of the model\n",
    "test_image = np.reshape(test_image, (64, 64))\n",
    "test_image = np.expand_dims(test_image, axis=-1)\n",
    "test_image = np.expand_dims(test_image, axis=0)\n",
    "\n",
    "cam = visualize_cam(model, test_image)\n",
    "\n",
    "# Plot the original image and the CAM\n",
    "plt.figure(figsize=(8, 4))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.imshow(test_image[0, :, :, 0], cmap='gray')\n",
    "plt.title('Original Image')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.imshow(test_image[0, :, :, 0], cmap='gray')\n",
    "plt.imshow(cam, cmap='jet', alpha=0.5)\n",
    "plt.title('Class Activation Map')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Class Activation Maps (CAM) of 2nd Layer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def visualize_cam(model, img):\n",
    "    # Extract the last convolutional layer\n",
    "    last_conv_layer = model.get_layer('conv2d_3')\n",
    "    \n",
    "    # Define a new model that outputs the last conv layer and the model's predicted class probabilities\n",
    "    cam_model = tf.keras.Model(model.input, (last_conv_layer.output, model.output))\n",
    "    \n",
    "    # Calculate the gradients of the predicted class with respect to the output feature map of the last conv layer\n",
    "    with tf.GradientTape() as tape:\n",
    "        conv_outputs, predictions = cam_model(img)\n",
    "        predicted_class = tf.argmax(predictions[0])\n",
    "        gradient = tape.gradient(predictions[:, predicted_class], conv_outputs)\n",
    "    \n",
    "    # Compute the weights using global average pooling on the gradients\n",
    "    weights = tf.reduce_mean(gradient[0], axis=(0, 1))\n",
    "    \n",
    "    # Get the feature map values and resize them to match the input image size\n",
    "    feature_map = conv_outputs[0]\n",
    "    cam = np.dot(feature_map, weights)\n",
    "    cam = cv2.resize(cam, (img.shape[2], img.shape[1]))\n",
    "    cam = np.maximum(cam, 0)\n",
    "    cam = cam / np.max(cam)\n",
    "    \n",
    "    return cam\n",
    "\n",
    "\n",
    "# Choose an image from the test set\n",
    "test_image = X_test[3]\n",
    "\n",
    "# Reshape the image to match the input shape of the model\n",
    "test_image = np.reshape(test_image, (1, 64, 64, 1))\n",
    "\n",
    "# Generate the CAM for the chosen image\n",
    "cam = visualize_cam(model, test_image)\n",
    "\n",
    "# Plot the original image and the CAM\n",
    "plt.figure(figsize=(8, 4))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.imshow(test_image[0, :, :, 0], cmap='gray')\n",
    "plt.title('Original Image')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.imshow(test_image[0, :, :, 0], cmap='gray')\n",
    "plt.imshow(cam, cmap='jet', alpha=0.5)\n",
    "plt.title('Class Activation Map')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Grad-CAM:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Model\n",
    "\n",
    "# Select a sample image from your test set\n",
    "sample_image = X_test[0]\n",
    "\n",
    "# Create a model that maps the input image to the activations of the last conv layer as well as the output predictions\n",
    "grad_model = Model([model.inputs], [model.get_layer('conv2d_3').output, model.output])\n",
    "\n",
    "# Get the gradient of the top predicted class for our input image with respect to the activations of the last conv layer\n",
    "with tf.GradientTape() as tape:\n",
    "    last_conv_layer_output, preds = grad_model(np.expand_dims(sample_image, axis=0))\n",
    "    pred_index = np.argmax(preds[0])\n",
    "    class_channel = preds[:, pred_index]\n",
    "\n",
    "# This is the gradient of the output neuron (top predicted or chosen)\n",
    "# with regard to the output feature map of the last conv layer\n",
    "grads = tape.gradient(class_channel, last_conv_layer_output)\n",
    "\n",
    "# This is a vector where each entry is the mean intensity of the gradient over a specific feature map channel\n",
    "pooled_grads = K.mean(grads, axis=(0, 1, 2))\n",
    "\n",
    "# We multiply each channel in the feature map array by \"how important this channel is\"\n",
    "# with regard to the top predicted class\n",
    "last_conv_layer_output = last_conv_layer_output[0]\n",
    "heatmap = last_conv_layer_output @ pooled_grads[..., np.newaxis]\n",
    "heatmap = np.squeeze(heatmap)\n",
    "\n",
    "# Normalize the heatmap between 0 & 1 for visualization\n",
    "heatmap = np.maximum(heatmap, 0) / np.max(heatmap)\n",
    "\n",
    "# Show the heatmap\n",
    "plt.matshow(heatmap)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Maps:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_feature_maps(model, img):\n",
    "    # Extract the intermediate feature maps\n",
    "    layer_outputs = [layer.output for layer in model.layers if 'conv2d' in layer.name]\n",
    "    activation_model = tf.keras.Model(inputs=model.input, outputs=layer_outputs)\n",
    "    feature_maps = activation_model.predict(img)\n",
    "    \n",
    "    # Plot the feature maps\n",
    "    for layer, feature_map in zip(model.layers, feature_maps):\n",
    "        if 'conv2d' in layer.name:\n",
    "            plt.figure(figsize=(8, 4))\n",
    "            plt.imshow(feature_map[0, :, :, 0], cmap='gray')\n",
    "            plt.title(layer.name + ' Feature Map')\n",
    "            plt.show()\n",
    "\n",
    "# Visualize the feature maps for the chosen image\n",
    "visualize_feature_maps(model, test_image)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Filter Visualization:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Get the weights of the first convolution layer\n",
    "weights1 = model.layers[0].get_weights()[0]\n",
    "\n",
    "# Normalize the weights to be between 0 and 1\n",
    "weights1 = (weights1 - weights1.min())/(weights1.max()-weights1.min())\n",
    "\n",
    "# Plot the weights as images\n",
    "plt.figure(figsize=(10,10))\n",
    "for i in range(25):\n",
    "    plt.subplot(5,5,i+1)\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "    plt.grid(False)\n",
    "    plt.imshow(weights1[:,:,:,i], cmap='gray')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Activation Histograms:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# get the symbolic outputs of each \"key\" layer (we gave them unique names).\n",
    "layer_dict = dict([(layer.name, layer) for layer in model.layers])\n",
    "\n",
    "# Specify the layer you want to visualize\n",
    "layer_name = 'conv2d_1'  # Change this to the layer name you're interested in\n",
    "filter_index = 0  # Can be any integer from 0 to 511, as there are 512 filters in that layer\n",
    "\n",
    "# Grab a reference to the input of the model and the specified layer\n",
    "input_img = model.input\n",
    "layer_output = layer_dict[layer_name].output\n",
    "\n",
    "# Build a function that returns the activation values given the input picture\n",
    "activation_func = K.function([input_img], [layer_output])\n",
    "\n",
    "# Use the above function to compute the activations\n",
    "activations = activation_func([X_test[0].reshape(1, 64, 64, 1)])\n",
    "\n",
    "# Get histogram data\n",
    "activation_values = activations[0].flatten()\n",
    "\n",
    "# Create histogram\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.histplot(activation_values, bins=50, kde=True)\n",
    "plt.title(f'Activation Histogram for Layer: {layer_name}')\n",
    "plt.xlabel('Activation Value')\n",
    "plt.ylabel('Frequency')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for layer in model.layers:\n",
    "    print(layer.name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Learning Curves:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting the accuracy and loss\n",
    "plt.figure(figsize=(12, 4))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(history.history['accuracy'], label='Train Accuracy')\n",
    "plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(history.history['loss'], label='Training Loss')\n",
    "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Architecture Visualization:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from keras.utils import plot_model\n",
    "\n",
    "# Generate the plot\n",
    "plot_model(model, to_file='model_plot.png', show_shapes=True, show_layer_names=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Details on Image Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify the folder path containing the images\n",
    "folder_path = \"Img\"\n",
    "\n",
    "# Get a list of image file names in the folder\n",
    "image_files = os.listdir(folder_path)\n",
    "\n",
    "# Iterate over the image files\n",
    "for file_name in image_files:\n",
    "    # Construct the full file path\n",
    "    file_path = os.path.join(folder_path, file_name)\n",
    "\n",
    "    # Open the image file\n",
    "    image = Image.open(file_path)\n",
    "\n",
    "    # Get the pixel size of the image\n",
    "    width, height = image.size\n",
    "\n",
    "    # Print the pixel size\n",
    "    print(f\"Image: {file_name}, Width: {width}px, Height: {height}px\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tfgpu",
   "language": "python",
   "name": "tfgpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
