{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "conda activate TFgpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# conda activate PyTorchGPU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div align=\"center\">\n",
    "  <img src=\"DisplayIMG/Cartoon.png\" alt=\"Image Description\" />\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_cores = os.cpu_count()  # Get number of CPU cores\n",
    "num_cores_to_use = num_cores // 2  # Use half of the cores\n",
    "\n",
    "num_cores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_cores_to_use"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Full Model before updates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'current_directory' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mcurrent_directory\u001b[49m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'current_directory' is not defined"
     ]
    }
   ],
   "source": [
    "current_directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'os' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m current_directory \u001b[38;5;241m=\u001b[39m \u001b[43mos\u001b[49m\u001b[38;5;241m.\u001b[39mgetcwd()\n",
      "\u001b[1;31mNameError\u001b[0m: name 'os' is not defined"
     ]
    }
   ],
   "source": [
    "current_directory = os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import os\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import pandas as pd\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Conv2D, MaxPooling2D, Flatten, Dropout, BatchNormalization\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import glob\n",
    "from sklearn.metrics import classification_report\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from keras_tuner import BayesianOptimization, HyperParameters\n",
    "import pickle\n",
    "\n",
    "# Get the current directory\n",
    "current_directory = os.getcwd()\n",
    "\n",
    "# Specify the folder name where the images are located\n",
    "folder_name = \"img\"\n",
    "\n",
    "# Construct the folder path\n",
    "folder_path = os.path.join(current_directory, folder_name)\n",
    "\n",
    "# Specify the CSV file path containing image labels\n",
    "csv_file_path = os.path.join(current_directory, \"labels\", \"english-clean.csv\")\n",
    "\n",
    "# Read the CSV file into a DataFrame\n",
    "df = pd.read_csv(csv_file_path)\n",
    "\n",
    "# Create a dictionary where keys are filenames and values are labels\n",
    "labels_dict = df.set_index('image')['label'].to_dict()\n",
    "\n",
    "\n",
    "def load_images_from_folder(folder, labels_dict):\n",
    "    images = []\n",
    "    labels = []\n",
    "    for file_path in glob.glob(os.path.join(folder, \"*.png\")):\n",
    "        img = Image.open(file_path)\n",
    "        if img is not None:\n",
    "            img = img.convert('L')  # Convert image to grayscale\n",
    "            img = img.resize((64, 64))  # Resize the image                   Ask about this --- Size of image being read?\n",
    "            np_img = np.array(img)\n",
    "            images.append(np_img)\n",
    "            filename = os.path.basename(file_path)\n",
    "            label = labels_dict[filename]  # Get the label from the filename\n",
    "            labels.append(label)\n",
    "    return images, labels\n",
    "\n",
    "images, labels = load_images_from_folder(folder_path, labels_dict)\n",
    "\n",
    "le = preprocessing.LabelEncoder()\n",
    "numerical_labels = le.fit_transform(labels)  # Convert labels to integers\n",
    "\n",
    "images = np.array(images)  # Convert list of arrays to a single array\n",
    "mean = np.mean(images)\n",
    "std = np.std(images)\n",
    "images = (images - mean) / std  # Normalize pixel values\n",
    "images = images.reshape(-1, 64, 64, 1)  # Reshape array for CNN\n",
    "\n",
    "encoded_labels = to_categorical(numerical_labels)  # One-hot encode labels\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(images, encoded_labels, test_size=0.2, random_state=42,\n",
    "                                                    shuffle=True)\n",
    "\n",
    "# Data augmentation for training data      #Ask about this - Working with numbers/letters (thinking shear could be problamatic?)\n",
    "\n",
    "datagen = ImageDataGenerator(                                                     \n",
    "    rotation_range=35,  # Rotate images randomly by 20 degrees\n",
    "    width_shift_range=0.1,  # Shift images horizontally by 10% of the width\n",
    "    height_shift_range=0.1,  # Shift images vertically by 10% of the height\n",
    "    shear_range=0.0,  # Apply shear transformation with a shear intensity of 0.2\n",
    "    zoom_range=0.1,  # Apply zoom transformation with a zoom range of 0.2\n",
    "    horizontal_flip=False,  # Flip images horizontally\n",
    "    vertical_flip=False  # Do not flip images vertically\n",
    ")\n",
    "\n",
    "datagen.fit(X_train)\n",
    "\n",
    "# Specify the file path to save the best hyperparameters\n",
    "hyperparameters_file_path = 'best_hyperparameters.pkl'\n",
    "\n",
    "def build_model(hp):                              \n",
    "    model = Sequential()     \n",
    "                                \n",
    "\n",
    "    # First block of convolutions\n",
    "    model.add(Conv2D(hp.Int('conv_1_units', min_value=32, max_value=128, step=32),\n",
    "                     (3, 3),\n",
    "                     activation='relu',\n",
    "                     input_shape=(64, 64, 1)))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(MaxPooling2D((2, 2)))\n",
    "    model.add(Dropout(hp.Float('dropout_1', min_value=0.0, max_value=0.5, step=0.1)))\n",
    "\n",
    "    # Second block of convolutions\n",
    "    model.add(Conv2D(hp.Int('conv_2_units', min_value=64, max_value=256, step=64),\n",
    "                     (3, 3),\n",
    "                     activation='relu'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(MaxPooling2D((2, 2)))\n",
    "    model.add(Dropout(hp.Float('dropout_2', min_value=0.0, max_value=0.5, step=0.1)))\n",
    "\n",
    "    # Fully connected layers\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(hp.Int('dense_1_units', min_value=128, max_value=512, step=128), activation='relu'))\n",
    "    model.add(Dropout(hp.Float('dropout_3', min_value=0.0, max_value=0.5, step=0.1)))\n",
    "    model.add(Dense(len(le.classes_), activation='softmax'))\n",
    "\n",
    "    # Compile model\n",
    "    model.compile(optimizer=Adam(hp.Choice('learning_rate', values=[1e-2, 1e-3, 1e-4])),\n",
    "                  loss='categorical_crossentropy',\n",
    "                  metrics=['accuracy'])\n",
    "\n",
    "    return model\n",
    "\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=10)\n",
    "model_checkpoint = ModelCheckpoint('best_model.h5', monitor='val_accuracy', save_best_only=True)\n",
    "\n",
    "tuner = BayesianOptimization(                                          \n",
    "    build_model,  # The function to construct the model\n",
    "    objective='val_accuracy',  # The metric to be optimized\n",
    "    max_trials=5,  # The maximum number of iterations for tuning\n",
    "    executions_per_trial=1,  # The number of models that should be built and fit for each trial for robustness purposes\n",
    "    directory=os.path.normpath('C:/keras_tuning'),  # The path to the directory where the search results are stored\n",
    "    project_name='keras_tuner_demo',  # The name of the project. This will be the name of the subdirectory under `directory` where the results are saved\n",
    "    overwrite=True  # Whether or not to overwrite the project if it already exists\n",
    ")\n",
    "\n",
    "tuner.search_space_summary()\n",
    "\n",
    "tuner.search(\n",
    "    X_train,  # Training data\n",
    "    y_train,  # Training labels\n",
    "    epochs=5,  # The number of epochs for training\n",
    "    validation_data=(X_test, y_test),  # Validation data\n",
    "    callbacks=[early_stopping, model_checkpoint]  # Callbacks to be used during training\n",
    ")\n",
    "\n",
    "tuner.results_summary()\n",
    "\n",
    "# Save the best hyperparameters\n",
    "best_hyperparameters = tuner.get_best_hyperparameters()[0]\n",
    "with open(hyperparameters_file_path, 'wb') as file:\n",
    "    pickle.dump(best_hyperparameters.values, file)\n",
    "\n",
    "    # Load the saved hyperparameters\n",
    "with open(hyperparameters_file_path, 'rb') as file:\n",
    "    loaded_hyperparameters_dict = pickle.load(file)\n",
    "    \n",
    "# Create a new HyperParameters object and set the loaded hyperparameters\n",
    "loaded_hyperparameters = HyperParameters()\n",
    "loaded_hyperparameters.values = loaded_hyperparameters_dict\n",
    "\n",
    "# Build and train the model with the loaded hyperparameters and augmented data\n",
    "best_model = build_model(loaded_hyperparameters)\n",
    "history = best_model.fit(datagen.flow(X_train, y_train),  # Use the augmented data generator for training data\n",
    "                         steps_per_epoch=len(X_train) // 32,  # Adjust the steps per epoch based on augmented data size\n",
    "                         epochs=50,\n",
    "                         validation_data=(X_test, y_test),\n",
    "                         callbacks=[early_stopping, model_checkpoint])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Remove Hypertuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import cv2\n",
    "# import os\n",
    "# import numpy as np\n",
    "# from PIL import Image\n",
    "# import pandas as pd\n",
    "# from sklearn import preprocessing\n",
    "# from sklearn.model_selection import train_test_split\n",
    "# from tensorflow.keras.utils import to_categorical\n",
    "# from tensorflow.keras.models import Sequential\n",
    "# from tensorflow.keras.layers import Dense, Conv2D, MaxPooling2D, Flatten, Dropout, BatchNormalization\n",
    "# from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "# from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "# from tensorflow.keras.optimizers import Adam\n",
    "# import glob\n",
    "# from sklearn.metrics import classification_report\n",
    "# import matplotlib.pyplot as plt\n",
    "# import tensorflow as tf\n",
    "# import pickle\n",
    "\n",
    "# # Get the current directory\n",
    "# current_directory = os.getcwd()\n",
    "\n",
    "# # Specify the folder name where the images are located\n",
    "# folder_name = \"img\"\n",
    "\n",
    "# # Construct the folder path\n",
    "# folder_path = os.path.join(current_directory, folder_name)\n",
    "\n",
    "# # Specify the CSV file path containing image labels\n",
    "# csv_file_path = os.path.join(current_directory, \"labels\", \"english-clean.csv\")\n",
    "\n",
    "# # Read the CSV file into a DataFrame\n",
    "# df = pd.read_csv(csv_file_path)\n",
    "\n",
    "# # Create a dictionary where keys are filenames and values are labels\n",
    "# labels_dict = df.set_index('image')['label'].to_dict()\n",
    "\n",
    "\n",
    "# def load_images_from_folder(folder, labels_dict):\n",
    "#     images = []\n",
    "#     labels = []\n",
    "#     for file_path in glob.glob(os.path.join(folder, \"*.png\")):\n",
    "#         img = Image.open(file_path)\n",
    "#         if img is not None:\n",
    "#             img = img.convert('L')  # Convert image to grayscale\n",
    "#             img = img.resize((64, 64))  # Resize the image\n",
    "#             np_img = np.array(img)\n",
    "#             images.append(np_img)\n",
    "#             filename = os.path.basename(file_path)\n",
    "#             label = labels_dict[filename]  # Get the label from the filename\n",
    "#             labels.append(label)\n",
    "#     return images, labels\n",
    "\n",
    "# images, labels = load_images_from_folder(folder_path, labels_dict)\n",
    "\n",
    "# le = preprocessing.LabelEncoder()\n",
    "# numerical_labels = le.fit_transform(labels)  # Convert labels to integers\n",
    "\n",
    "# images = np.array(images)  # Convert list of arrays to a single array\n",
    "# mean = np.mean(images)\n",
    "# std = np.std(images)\n",
    "# images = (images - mean) / std  # Normalize pixel values\n",
    "# images = images.reshape(-1, 64, 64, 1)  # Reshape array for CNN\n",
    "\n",
    "# encoded_labels = to_categorical(numerical_labels)  # One-hot encode labels\n",
    "\n",
    "# X_train, X_test, y_train, y_test = train_test_split(images, encoded_labels, test_size=0.2, random_state=42,\n",
    "#                                                     shuffle=True)\n",
    "\n",
    "# # Data augmentation for training data\n",
    "# datagen = ImageDataGenerator(                                                     \n",
    "#     rotation_range=35,  # Rotate images randomly by 20 degrees\n",
    "#     width_shift_range=0.1,  # Shift images horizontally by 10% of the width\n",
    "#     height_shift_range=0.1,  # Shift images vertically by 10% of the height\n",
    "#     shear_range=0.0,  # Apply shear transformation with a shear intensity of 0.2\n",
    "#     zoom_range=0.1,  # Apply zoom transformation with a zoom range of 0.2\n",
    "#     horizontal_flip=False,  # Flip images horizontally\n",
    "#     vertical_flip=False  # Do not flip images vertically\n",
    "# )\n",
    "\n",
    "# datagen.fit(X_train)\n",
    "\n",
    "# # Build the model\n",
    "# model = Sequential()     \n",
    "\n",
    "# # First block of convolutions\n",
    "# model.add(Conv2D(32, (3, 3), activation='relu', input_shape=(64, 64, 1)))\n",
    "# model.add(BatchNormalization())\n",
    "# model.add(MaxPooling2D((2, 2)))\n",
    "# model.add(Dropout(0.25))\n",
    "\n",
    "# # Second block of convolutions\n",
    "# model.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "# model.add(BatchNormalization())\n",
    "# model.add(MaxPooling2D((2, 2)))\n",
    "# model.add(Dropout(0.25))\n",
    "\n",
    "# # Fully connected layers\n",
    "# model.add(Flatten())\n",
    "# model.add(Dense(128, activation='relu'))\n",
    "# model.add(Dropout(0.5))\n",
    "# model.add(Dense(len(le.classes_), activation='softmax'))\n",
    "\n",
    "# # Compile the model\n",
    "# model.compile(optimizer=Adam(learning_rate=0.001),\n",
    "#               loss='categorical_crossentropy',\n",
    "#               metrics=['accuracy'])\n",
    "\n",
    "# # Set up callbacks\n",
    "# early_stopping = EarlyStopping(monitor='val_loss', patience=10)\n",
    "# model_checkpoint = ModelCheckpoint('best_model.h5', monitor='val_accuracy', save_best_only=True)\n",
    "\n",
    "# # Train the model\n",
    "# history = model.fit(datagen.flow(X_train, y_train),\n",
    "#                     steps_per_epoch=len(X_train) // 32,\n",
    "#                     epochs=50,\n",
    "#                     validation_data=(X_test, y_test),\n",
    "#                     callbacks=[early_stopping, model_checkpoint])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VGG16 Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please install GPU version of TF\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "if tf.test.gpu_device_name():\n",
    "    print('Default GPU Device: {}'.format(tf.test.gpu_device_name()))\n",
    "else:\n",
    "    print(\"Please install GPU version of TF\")\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "85/85 [==============================] - 231s 3s/step - loss: 6.0157 - accuracy: 0.0126 - val_loss: 4.1294 - val_accuracy: 0.0073\n",
      "Epoch 2/50\n",
      "85/85 [==============================] - 223s 3s/step - loss: 4.1276 - accuracy: 0.0148 - val_loss: 4.1307 - val_accuracy: 0.0073\n",
      "Epoch 3/50\n",
      "85/85 [==============================] - 222s 3s/step - loss: 4.1272 - accuracy: 0.0156 - val_loss: 4.1319 - val_accuracy: 0.0073\n",
      "Epoch 4/50\n",
      "85/85 [==============================] - 225s 3s/step - loss: 4.1271 - accuracy: 0.0189 - val_loss: 4.1338 - val_accuracy: 0.0088\n",
      "Epoch 5/50\n",
      "85/85 [==============================] - 227s 3s/step - loss: 4.1270 - accuracy: 0.0182 - val_loss: 4.1344 - val_accuracy: 0.0073\n",
      "Epoch 6/50\n",
      "85/85 [==============================] - 222s 3s/step - loss: 4.1267 - accuracy: 0.0167 - val_loss: 4.1362 - val_accuracy: 0.0073\n",
      "Epoch 7/50\n",
      "85/85 [==============================] - 223s 3s/step - loss: 4.1264 - accuracy: 0.0156 - val_loss: 4.1370 - val_accuracy: 0.0073\n",
      "Epoch 8/50\n",
      "85/85 [==============================] - 222s 3s/step - loss: 4.1265 - accuracy: 0.0156 - val_loss: 4.1385 - val_accuracy: 0.0073\n",
      "Epoch 9/50\n",
      "85/85 [==============================] - 222s 3s/step - loss: 4.1263 - accuracy: 0.0185 - val_loss: 4.1401 - val_accuracy: 0.0073\n",
      "Epoch 10/50\n",
      "85/85 [==============================] - 222s 3s/step - loss: 4.1262 - accuracy: 0.0185 - val_loss: 4.1414 - val_accuracy: 0.0073\n",
      "Epoch 11/50\n",
      "85/85 [==============================] - 220s 3s/step - loss: 4.1264 - accuracy: 0.0185 - val_loss: 4.1411 - val_accuracy: 0.0073\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import os\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import pandas as pd\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Flatten, Dropout, Conv2D, MaxPooling2D\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import glob\n",
    "from sklearn.metrics import classification_report\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "\n",
    "# Get the current directory\n",
    "current_directory = os.getcwd()\n",
    "\n",
    "# Specify the folder name where the images are located\n",
    "folder_name = \"img\"\n",
    "\n",
    "# Construct the folder path\n",
    "folder_path = os.path.join(current_directory, folder_name)\n",
    "\n",
    "# Specify the CSV file path containing image labels\n",
    "csv_file_path = os.path.join(current_directory, \"labels\", \"english-clean.csv\")\n",
    "\n",
    "# Read the CSV file into a DataFrame\n",
    "df = pd.read_csv(csv_file_path)\n",
    "\n",
    "# Create a dictionary where keys are filenames and values are labels\n",
    "labels_dict = df.set_index('image')['label'].to_dict()\n",
    "\n",
    "\n",
    "def load_images_from_folder(folder, labels_dict):\n",
    "    images = []\n",
    "    labels = []\n",
    "    for file_path in glob.glob(os.path.join(folder, \"*.png\")):\n",
    "        img = Image.open(file_path)\n",
    "        if img is not None:\n",
    "            img = img.resize((224, 224))  # Resize the image to match VGG16 input size\n",
    "            np_img = np.array(img)\n",
    "            images.append(np_img)\n",
    "            filename = os.path.basename(file_path)\n",
    "            label = labels_dict[filename]  # Get the label from the filename\n",
    "            labels.append(label)\n",
    "    return images, labels\n",
    "\n",
    "images, labels = load_images_from_folder(folder_path, labels_dict)\n",
    "\n",
    "le = preprocessing.LabelEncoder()\n",
    "numerical_labels = le.fit_transform(labels)  # Convert labels to integers\n",
    "\n",
    "images = np.array(images)  # Convert list of arrays to a single array\n",
    "mean = np.mean(images)\n",
    "std = np.std(images)\n",
    "images = (images - mean) / std  # Normalize pixel values\n",
    "images = images.reshape(-1, 224, 224, 3)  # Reshape array for VGG16 (RGB input)\n",
    "\n",
    "encoded_labels = to_categorical(numerical_labels)  # One-hot encode labels\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(images, encoded_labels, test_size=0.2, random_state=42,\n",
    "                                                    shuffle=True)\n",
    "\n",
    "# Data augmentation for training data\n",
    "datagen = ImageDataGenerator(\n",
    "    rotation_range=35,  # Rotate images randomly by 20 degrees\n",
    "    width_shift_range=0.1,  # Shift images horizontally by 10% of the width\n",
    "    height_shift_range=0.1,  # Shift images vertically by 10% of the height\n",
    "    shear_range=0.0,  # Apply shear transformation with a shear intensity of 0.2\n",
    "    zoom_range=0.1,  # Apply zoom transformation with a zoom range of 0.2\n",
    "    horizontal_flip=False,  # Flip images horizontally\n",
    "    vertical_flip=False  # Do not flip images vertically\n",
    ")\n",
    "\n",
    "datagen.fit(X_train)\n",
    "\n",
    "# Define the VGG16 model structure\n",
    "def create_model(input_shape, num_classes):\n",
    "    model = Sequential()\n",
    "\n",
    "    # Block 1\n",
    "    model.add(Conv2D(64, (3, 3), activation='relu', padding='same', input_shape=input_shape))\n",
    "    model.add(Conv2D(64, (3, 3), activation='relu', padding='same'))\n",
    "    model.add(MaxPooling2D((2, 2), strides=(2, 2)))\n",
    "\n",
    "    # Block 2\n",
    "    model.add(Conv2D(128, (3, 3), activation='relu', padding='same'))\n",
    "    model.add(Conv2D(128, (3, 3), activation='relu', padding='same'))\n",
    "    model.add(MaxPooling2D((2, 2), strides=(2, 2)))\n",
    "\n",
    "    # Block 3\n",
    "    model.add(Conv2D(256, (3, 3), activation='relu', padding='same'))\n",
    "    model.add(Conv2D(256, (3, 3), activation='relu', padding='same'))\n",
    "    model.add(Conv2D(256, (3, 3), activation='relu', padding='same'))\n",
    "    model.add(MaxPooling2D((2, 2), strides=(2, 2)))\n",
    "\n",
    "    # Block 4\n",
    "    model.add(Conv2D(512, (3, 3), activation='relu', padding='same'))\n",
    "    model.add(Conv2D(512, (3, 3), activation='relu', padding='same'))\n",
    "    model.add(Conv2D(512, (3, 3), activation='relu', padding='same'))\n",
    "    model.add(MaxPooling2D((2, 2), strides=(2, 2)))\n",
    "\n",
    "    # Block 5\n",
    "    model.add(Conv2D(512, (3, 3), activation='relu', padding='same'))\n",
    "    model.add(Conv2D(512, (3, 3), activation='relu', padding='same'))\n",
    "    model.add(Conv2D(512, (3, 3), activation='relu', padding='same'))\n",
    "    model.add(MaxPooling2D((2, 2), strides=(2, 2)))\n",
    "\n",
    "    # Classification block\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(4096, activation='relu'))\n",
    "    model.add(Dense(4096, activation='relu'))\n",
    "    model.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "    return model\n",
    "\n",
    "input_shape = (224, 224, 3)\n",
    "num_classes = len(le.classes_)\n",
    "model = create_model(input_shape, num_classes)\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer=Adam(learning_rate=0.001),\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Set up callbacks\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=10)\n",
    "model_checkpoint = ModelCheckpoint('best_model.h5', monitor='val_accuracy', save_best_only=True)\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(datagen.flow(X_train, y_train),\n",
    "                    steps_per_epoch=len(X_train) // 32,\n",
    "                    epochs=50,\n",
    "                    validation_data=(X_test, y_test),\n",
    "                    callbacks=[early_stopping, model_checkpoint])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import torch\n",
    "import numpy as np\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (2608241997.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn[3], line 1\u001b[1;36m\u001b[0m\n\u001b[1;33m    python \"import torch; print(torch.cuda.is_available())\"\u001b[0m\n\u001b[1;37m           ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "python -c \"import torch; print(torch.cuda.is_available())\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# import glob\n",
    "# import torch\n",
    "# import numpy as np\n",
    "# from PIL import Image\n",
    "# import pandas as pd\n",
    "# from sklearn import preprocessing\n",
    "# from sklearn.model_selection import train_test_split\n",
    "# import torch.nn as nn\n",
    "# import torch.optim as optim\n",
    "# from torch.utils.data import DataLoader, Dataset\n",
    "# from torchvision import transforms\n",
    "# from torch.autograd import Variable\n",
    "\n",
    "# # Get the current directory\n",
    "# current_directory = os.getcwd()\n",
    "\n",
    "# # Specify the folder name where the images are located\n",
    "# folder_name = \"img\"\n",
    "\n",
    "# # Construct the folder path\n",
    "# folder_path = os.path.join(current_directory, folder_name)\n",
    "\n",
    "# # Specify the CSV file path containing image labels\n",
    "# csv_file_path = os.path.join(current_directory, \"labels\", \"english-clean.csv\")\n",
    "\n",
    "# # Read the CSV file into a DataFrame\n",
    "# df = pd.read_csv(csv_file_path)\n",
    "\n",
    "# # Create a dictionary where keys are filenames and values are labels\n",
    "# labels_dict = df.set_index('image')['label'].to_dict()\n",
    "\n",
    "# def load_images_from_folder(folder, labels_dict):\n",
    "#     images = []\n",
    "#     labels = []\n",
    "#     for file_path in glob.glob(os.path.join(folder, \"*.png\")):\n",
    "#         img = Image.open(file_path)\n",
    "#         if img is not None:\n",
    "#             img = img.resize((224, 224))  # Resize the image to match VGG16 input size\n",
    "#             images.append(np.array(img))\n",
    "#             filename = os.path.basename(file_path)\n",
    "#             label = labels_dict[filename]  # Get the label from the filename\n",
    "#             labels.append(label)\n",
    "#     return images, labels\n",
    "\n",
    "# images, labels = load_images_from_folder(folder_path, labels_dict)\n",
    "\n",
    "# le = preprocessing.LabelEncoder()\n",
    "# numerical_labels = le.fit_transform(labels)  # Convert labels to integers\n",
    "\n",
    "# images = np.array(images)  # Convert list of arrays to a single array\n",
    "# images = images / 255.0  # Normalize pixel values to [0, 1]\n",
    "\n",
    "# X_train, X_test, y_train, y_test = train_test_split(images, numerical_labels, test_size=0.2, random_state=42,\n",
    "#                                                     shuffle=True)\n",
    "\n",
    "# X_train = torch.from_numpy(X_train).permute(0, 3, 1, 2).float()  # Convert to PyTorch tensors\n",
    "# X_test = torch.from_numpy(X_test).permute(0, 3, 1, 2).float()\n",
    "# y_train = torch.from_numpy(y_train).long()\n",
    "# y_test = torch.from_numpy(y_test).long()\n",
    "\n",
    "# # Define the VGG16 model structure\n",
    "# class VGG16(nn.Module):\n",
    "#     def __init__(self, num_classes):\n",
    "#         super(VGG16, self).__init__()\n",
    "\n",
    "#         self.features = nn.Sequential(\n",
    "#             # Block 1\n",
    "#             nn.Conv2d(3, 64, kernel_size=3, padding=1),\n",
    "#             nn.ReLU(inplace=True),\n",
    "#             nn.Conv2d(64, 64, kernel_size=3, padding=1),\n",
    "#             nn.ReLU(inplace=True),\n",
    "#             nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "\n",
    "#             # Block 2\n",
    "#             nn.Conv2d(64, 128, kernel_size=3, padding=1),\n",
    "#             nn.ReLU(inplace=True),\n",
    "#             nn.Conv2d(128, 128, kernel_size=3, padding=1),\n",
    "#             nn.ReLU(inplace=True),\n",
    "#             nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "\n",
    "#             # Block 3\n",
    "#             nn.Conv2d(128, 256, kernel_size=3, padding=1),\n",
    "#             nn.ReLU(inplace=True),\n",
    "#             nn.Conv2d(256, 256, kernel_size=3, padding=1),\n",
    "#             nn.ReLU(inplace=True),\n",
    "#             nn.Conv2d(256, 256, kernel_size=3, padding=1),\n",
    "#             nn.ReLU(inplace=True),\n",
    "#             nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "\n",
    "#             # Block 4\n",
    "#             nn.Conv2d(256, 512, kernel_size=3, padding=1),\n",
    "#             nn.ReLU(inplace=True),\n",
    "#             nn.Conv2d(512, 512, kernel_size=3, padding=1),\n",
    "#             nn.ReLU(inplace=True),\n",
    "#             nn.Conv2d(512, 512, kernel_size=3, padding=1),\n",
    "#             nn.ReLU(inplace=True),\n",
    "#             nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "\n",
    "#             # Block 5\n",
    "#             nn.Conv2d(512, 512, kernel_size=3, padding=1),\n",
    "#             nn.ReLU(inplace=True),\n",
    "#             nn.Conv2d(512, 512, kernel_size=3, padding=1),\n",
    "#             nn.ReLU(inplace=True),\n",
    "#             nn.Conv2d(512, 512, kernel_size=3, padding=1),\n",
    "#             nn.ReLU(inplace=True),\n",
    "#             nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "#         )\n",
    "\n",
    "#         self.classifier = nn.Sequential(\n",
    "#             nn.Linear(512 * 7 * 7, 4096),\n",
    "#             nn.ReLU(True),\n",
    "#             nn.Dropout(),\n",
    "#             nn.Linear(4096, 4096),\n",
    "#             nn.ReLU(True),\n",
    "#             nn.Dropout(),\n",
    "#             nn.Linear(4096, num_classes),\n",
    "#         )\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         x = self.features(x)\n",
    "#         x = x.view(x.size(0), -1)\n",
    "#         x = self.classifier(x)\n",
    "#         return x\n",
    "\n",
    "# model = VGG16(len(le.classes_))\n",
    "\n",
    "# # Define the loss function and the optimizer\n",
    "# criterion = nn.CrossEntropyLoss()\n",
    "# optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# # Training loop\n",
    "# for epoch in range(50):  # loop over the dataset multiple times\n",
    "#     running_loss = 0.0\n",
    "#     for i, data in enumerate(zip(X_train, y_train), 0):\n",
    "#         # get the inputs; data is a list of [inputs, labels]\n",
    "#         inputs, labels = data\n",
    "\n",
    "#         # wrap them in Variable\n",
    "#         inputs, labels = Variable(inputs.unsqueeze(0)), Variable(labels.unsqueeze(0))\n",
    "\n",
    "#         # zero the parameter gradients\n",
    "#         optimizer.zero_grad()\n",
    "\n",
    "#         # forward + backward + optimize\n",
    "#         outputs = model(inputs)\n",
    "#         loss = criterion(outputs, labels)\n",
    "#         loss.backward()\n",
    "#         optimizer.step()\n",
    "\n",
    "#         # print statistics\n",
    "#         running_loss += loss.item()\n",
    "#         if i % 2000 == 1999:  # print every 2000 mini-batches\n",
    "#             print('[%d, %5d] loss: %.3f' %\n",
    "#                   (epoch + 1, i + 1, running_loss / 2000))\n",
    "#             running_loss = 0.0\n",
    "\n",
    "# print('Finished Training')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "CUDA_LAUNCH_BLOCKING=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1,  2000] loss: 4.143\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: an illegal memory access was encountered\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 175\u001b[0m\n\u001b[0;32m    173\u001b[0m outputs \u001b[38;5;241m=\u001b[39m model(X_test\u001b[38;5;241m.\u001b[39mto(device))\n\u001b[0;32m    174\u001b[0m _, predicted \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mmax(outputs\u001b[38;5;241m.\u001b[39mdata, \u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m--> 175\u001b[0m correct \u001b[38;5;241m=\u001b[39m (predicted \u001b[38;5;241m==\u001b[39m \u001b[43my_test\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m)\u001b[38;5;241m.\u001b[39msum()\u001b[38;5;241m.\u001b[39mitem()\n\u001b[0;32m    176\u001b[0m accuracy \u001b[38;5;241m=\u001b[39m correct \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mlen\u001b[39m(y_test) \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m100\u001b[39m\n\u001b[0;32m    177\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mValidation Accuracy: \u001b[39m\u001b[38;5;132;01m%.2f\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m%%\u001b[39;00m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m%\u001b[39m accuracy, flush\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: CUDA error: an illegal memory access was encountered\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import glob\n",
    "import torch\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import pandas as pd\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# Enable device-side assertions\n",
    "os.environ[\"TORCH_USE_CUDA_DSA\"] = \"1\"\n",
    "\n",
    "# Get the current directory\n",
    "current_directory = os.getcwd()\n",
    "\n",
    "# Specify the folder name where the images are located\n",
    "folder_name = \"img\"\n",
    "\n",
    "# Construct the folder path\n",
    "folder_path = os.path.join(current_directory, folder_name)\n",
    "\n",
    "# Specify the CSV file path containing image labels\n",
    "csv_file_path = os.path.join(current_directory, \"labels\", \"english-clean.csv\")\n",
    "\n",
    "# Read the CSV file into a DataFrame\n",
    "df = pd.read_csv(csv_file_path)\n",
    "\n",
    "# Create a dictionary where keys are filenames and values are labels\n",
    "labels_dict = df.set_index('image')['label'].to_dict()\n",
    "\n",
    "def load_images_from_folder(folder, labels_dict):\n",
    "    images = []\n",
    "    labels = []\n",
    "    for file_path in glob.glob(os.path.join(folder, \"*.png\")):\n",
    "        img = Image.open(file_path)\n",
    "        if img is not None:\n",
    "            img = img.resize((224, 224))  # Resize the image to match VGG16 input size\n",
    "            images.append(np.array(img))\n",
    "            filename = os.path.basename(file_path)\n",
    "            label = labels_dict[filename]  # Get the label from the filename\n",
    "            labels.append(label)\n",
    "    return images, labels\n",
    "\n",
    "images, labels = load_images_from_folder(folder_path, labels_dict)\n",
    "\n",
    "le = preprocessing.LabelEncoder()\n",
    "numerical_labels = le.fit_transform(labels)  # Convert labels to integers\n",
    "\n",
    "images = np.array(images)  # Convert list of arrays to a single array\n",
    "images = images / 255.0  # Normalize pixel values to [0, 1]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(images, numerical_labels, test_size=0.2, random_state=42,\n",
    "                                                    shuffle=True)\n",
    "\n",
    "X_train = torch.from_numpy(X_train).permute(0, 3, 1, 2).float()  # Convert to PyTorch tensors\n",
    "X_test = torch.from_numpy(X_test).permute(0, 3, 1, 2).float()\n",
    "y_train = torch.from_numpy(y_train).long()\n",
    "y_test = torch.from_numpy(y_test).long()\n",
    "\n",
    "# Define the VGG16 model structure\n",
    "class VGG16(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super(VGG16, self).__init__()\n",
    "\n",
    "        self.features = nn.Sequential(\n",
    "            # Block 1\n",
    "            nn.Conv2d(3, 64, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(64, 64, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "\n",
    "            # Block 2\n",
    "            nn.Conv2d(64, 128, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(128, 128, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "\n",
    "            # Block 3\n",
    "            nn.Conv2d(128, 256, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(256, 256, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(256, 256, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "\n",
    "            # Block 4\n",
    "            nn.Conv2d(256, 512, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(512, 512, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(512, 512, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "\n",
    "            # Block 5\n",
    "            nn.Conv2d(512, 512, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(512, 512, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(512, 512, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "        )\n",
    "\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(512 * 7 * 7, 4096),\n",
    "            nn.ReLU(True),\n",
    "            nn.Dropout(),\n",
    "            nn.Linear(4096, 4096),\n",
    "            nn.ReLU(True),\n",
    "            nn.Dropout(),\n",
    "            nn.Linear(4096, num_classes),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = x.flatten(start_dim=1)  # Flatten the tensor\n",
    "        x = self.classifier(x)\n",
    "        return x\n",
    "\n",
    "model = VGG16(len(le.classes_))\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "# Define the loss function and the optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Set CUDA_LAUNCH_BLOCKING=1 for synchronous CUDA error reporting\n",
    "os.environ[\"CUDA_LAUNCH_BLOCKING\"] = \"1\"\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(50):  # loop over the dataset multiple times\n",
    "    running_loss = 0.0\n",
    "    total_correct = 0\n",
    "    total_samples = 0\n",
    "\n",
    "    for i, data in enumerate(zip(X_train, y_train), 0):\n",
    "        # get the inputs; data is a list of [inputs, labels]\n",
    "        inputs, labels = data\n",
    "\n",
    "        # move them to the same device as the model\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward + backward + optimize\n",
    "        outputs = model(inputs.unsqueeze(0))\n",
    "        loss = criterion(outputs, labels.unsqueeze(0))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # print statistics\n",
    "        running_loss += loss.item()\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total_samples += 1  # Increment total_samples by 1\n",
    "\n",
    "        total_correct += (predicted == labels).sum().item()\n",
    "\n",
    "        if i % 2000 == 1999:  # print every 2000 mini-batches\n",
    "            print('[%d, %5d] loss: %.3f' % (epoch + 1, i + 1, running_loss / 2000), flush=True)\n",
    "            running_loss = 0.0\n",
    "\n",
    "    # Calculate and print validation accuracy after each epoch\n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "        outputs = model(X_test.to(device))\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "\n",
    "        print(\"Debug: Predicted shape\", predicted.shape)\n",
    "        print(\"Debug: y_test shape\", y_test.shape)\n",
    "        print(\"Debug: Predicted device\", predicted.device)\n",
    "        print(\"Debug: y_test device\", y_test.device)\n",
    "\n",
    "        try:\n",
    "            correct = (predicted == y_test.to(device)).sum().item()\n",
    "        except Exception as e:\n",
    "            print(\"Debug: Exception caught\", str(e))\n",
    "            raise\n",
    "\n",
    "        accuracy = correct / len(y_test) * 100\n",
    "        print('Validation Accuracy: %.2f %%' % accuracy, flush=True)\n",
    "        model.train()\n",
    "\n",
    "print('Finished Training')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question - ways to evaluate the layers of the model?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1200x400 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "NameError",
     "evalue": "name 'best_model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 22\u001b[0m\n\u001b[0;32m     19\u001b[0m plt\u001b[38;5;241m.\u001b[39mshow()\n\u001b[0;32m     21\u001b[0m \u001b[38;5;66;03m# Make predictions on the test set\u001b[39;00m\n\u001b[1;32m---> 22\u001b[0m y_pred \u001b[38;5;241m=\u001b[39m \u001b[43mbest_model\u001b[49m\u001b[38;5;241m.\u001b[39mpredict(X_test)\n\u001b[0;32m     23\u001b[0m y_pred_classes \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39margmax(y_pred, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m     24\u001b[0m y_true_classes \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39margmax(y_test, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'best_model' is not defined"
     ]
    }
   ],
   "source": [
    "# Plotting the accuracy and loss\n",
    "plt.figure(figsize=(12, 4))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(history.history['accuracy'], label='Train Accuracy')\n",
    "plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(history.history['loss'], label='Training Loss')\n",
    "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = best_model.predict(X_test)\n",
    "y_pred_classes = np.argmax(y_pred, axis=1)\n",
    "y_true_classes = np.argmax(y_test, axis=1)\n",
    "\n",
    "# Print a classification report\n",
    "print(classification_report(y_true_classes, y_pred_classes, target_names=le.classes_, zero_division=1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Plot training & validation accuracy values\n",
    "plt.figure(figsize=[12,8])\n",
    "plt.plot(history.history['accuracy'])\n",
    "plt.plot(history.history['val_accuracy'])\n",
    "plt.title('Model accuracy')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Test'], loc='upper left')\n",
    "plt.show()\n",
    "\n",
    "# Plot training & validation loss values\n",
    "plt.figure(figsize=[12,8])\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('Model loss')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Test'], loc='upper left')\n",
    "plt.show()\n",
    "\n",
    "# Evaluate the model on the test data using `evaluate`\n",
    "print(\"Evaluate on test data\")\n",
    "results = best_model.evaluate(X_test, y_test, batch_size=128)\n",
    "print(\"test loss, test accuracy:\", results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# #Save Model For Furture Use\n",
    "# model.save('EpochTenThousand')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pickle\n",
    "\n",
    "# # Assume you have a history object from model.fit\n",
    "# # history = model.fit(....)\n",
    "\n",
    "# # Save it under some name\n",
    "# with open('trainHistoryDict', 'wb') as file_pi:\n",
    "#     pickle.dump(history.history, file_pi)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open('trainHistoryDict', 'rb') as file_pi:\n",
    "#     loaded_history = pickle.load(file_pi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model on the training set\n",
    "train_loss, train_accuracy = best_model.evaluate(X_train, y_train)\n",
    "print(\"Train Loss:\", train_loss)\n",
    "print(\"Train Accuracy:\", train_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Evaluate the model on the test set\n",
    "loss, accuracy = best_model.evaluate(X_test, y_test)\n",
    "print(\"Test Loss:\", loss)\n",
    "print(\"Test Accuracy:\", accuracy)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = best_model.predict(X_test)\n",
    "y_pred_labels = np.argmax(y_pred, axis=1)\n",
    "y_true_labels = np.argmax(y_test, axis=1)\n",
    "\n",
    "# Print classification report\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_true_labels, y_pred_labels, zero_division=1))\n",
    "\n",
    "# Print confusion matrix\n",
    "from sklearn.metrics import confusion_matrix\n",
    "confusion_mtx = confusion_matrix(y_true_labels, y_pred_labels)\n",
    "print(\"Confusion Matrix:\")\n",
    "print(confusion_mtx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = best_model.predict(X_test)\n",
    "\n",
    "# Convert probabilities to labels\n",
    "y_pred_labels = np.argmax(y_pred, axis=1)\n",
    "\n",
    "# Also convert the one-hot encoded labels back to label encoding\n",
    "y_true_labels = np.argmax(y_test, axis=1)\n",
    "\n",
    "# Convert numerical labels back to original labels\n",
    "y_pred_labels = le.inverse_transform(y_pred_labels)\n",
    "y_true_labels = le.inverse_transform(y_true_labels)\n",
    "\n",
    "# Calculate confusion matrix\n",
    "confusion_mtx = confusion_matrix(y_true_labels, y_pred_labels)\n",
    "\n",
    "# Create a list of unique labels\n",
    "labels = list(le.classes_)\n",
    "\n",
    "# Set the font scale (this will affect heatmap annotation size)\n",
    "sns.set(font_scale=1.5)\n",
    "\n",
    "# Adjust size of labels, title using rcParams\n",
    "plt.rcParams['xtick.labelsize']=15\n",
    "plt.rcParams['ytick.labelsize']=15\n",
    "\n",
    "# Visualize confusion matrix using seaborn's heatmap\n",
    "plt.figure(figsize=(25, 20))\n",
    "sns.heatmap(confusion_mtx, annot=True, fmt='d', cmap='Blues', xticklabels=labels, yticklabels=labels)\n",
    "plt.xlabel('Predicted Label', fontsize=20)\n",
    "plt.ylabel('True Label', fontsize=20)\n",
    "plt.title('Confusion Matrix', fontsize=25)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "best_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Class Activation Maps (CAM):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras.backend as K\n",
    "\n",
    "def visualize_cam(model, img):\n",
    "    # Extract the last convolutional layer\n",
    "    last_conv_layer = model.get_layer('conv2d_2')\n",
    "    \n",
    "    # Define a new model that outputs the last conv layer and the model's predicted class probabilities\n",
    "    cam_model = tf.keras.Model(model.input, (last_conv_layer.output, model.output))\n",
    "    \n",
    "    # Calculate the gradients of the predicted class with respect to the output feature map of the last conv layer\n",
    "    with tf.GradientTape() as tape:\n",
    "        conv_outputs, predictions = cam_model(img)\n",
    "        predicted_class = tf.argmax(predictions[0])\n",
    "        gradient = tape.gradient(predictions[:, predicted_class], conv_outputs)\n",
    "    \n",
    "    # Compute the weights using global average pooling on the gradients\n",
    "    weights = tf.reduce_mean(gradient[0], axis=(0, 1))\n",
    "    \n",
    "    # Get the feature map values and resize them to match the input image size\n",
    "    feature_map = conv_outputs[0]\n",
    "    cam = np.dot(feature_map, weights)\n",
    "    cam = cv2.resize(cam, (img.shape[2], img.shape[1]))\n",
    "    cam = np.maximum(cam, 0)\n",
    "    \n",
    "    # Normalize the CAM values if the maximum value is non-zero\n",
    "    max_value = np.max(cam)\n",
    "    if max_value != 0:\n",
    "        cam = cam / max_value\n",
    "    \n",
    "    return cam\n",
    "\n",
    "\n",
    "# Choose an image from the test set\n",
    "test_image = X_test[4]\n",
    "\n",
    "# Reshape the image to match the input shape of the model\n",
    "test_image = np.reshape(test_image, (1, 64, 64, 1))\n",
    "\n",
    "# Generate the CAM for the chosen image\n",
    "cam = visualize_cam(best_model, test_image)\n",
    "\n",
    "# Plot the original image and the CAM\n",
    "plt.figure(figsize=(8, 4))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.imshow(test_image[0, :, :, 0], cmap='gray')\n",
    "plt.title('Original Image')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.imshow(test_image[0, :, :, 0], cmap='gray')\n",
    "plt.imshow(cam, cmap='jet', alpha=0.5)\n",
    "plt.title('Class Activation Map')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def visualize_cam(model, img):\n",
    "    # Extract the last convolutional layer\n",
    "    last_conv_layer = model.get_layer('conv2d_3')\n",
    "    \n",
    "    # Define a new model that outputs the last conv layer and the model's predicted class probabilities\n",
    "    cam_model = tf.keras.Model(model.input, (last_conv_layer.output, model.output))\n",
    "    \n",
    "    # Calculate the gradients of the predicted class with respect to the output feature map of the last conv layer\n",
    "    with tf.GradientTape() as tape:\n",
    "        conv_outputs, predictions = cam_model(img)\n",
    "        predicted_class = tf.argmax(predictions[0])\n",
    "        gradient = tape.gradient(predictions[:, predicted_class], conv_outputs)\n",
    "    \n",
    "    # Compute the weights using global average pooling on the gradients\n",
    "    weights = tf.reduce_mean(gradient[0], axis=(0, 1))\n",
    "    \n",
    "    # Get the feature map values and resize them to match the input image size\n",
    "    feature_map = conv_outputs[0]\n",
    "    cam = np.dot(feature_map, weights)\n",
    "    cam = cv2.resize(cam, (img.shape[2], img.shape[1]))\n",
    "    cam = np.maximum(cam, 0)\n",
    "    cam = cam / np.max(cam)\n",
    "    \n",
    "    return cam\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "cam = visualize_cam(best_model, test_image)\n",
    "\n",
    "# Plot the original image and the CAM\n",
    "plt.figure(figsize=(8, 4))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.imshow(test_image[0, :, :, 0], cmap='gray')\n",
    "plt.title('Original Image')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.imshow(test_image[0, :, :, 0], cmap='gray')\n",
    "plt.imshow(cam, cmap='jet', alpha=0.5)\n",
    "plt.title('Class Activation Map')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras.backend as K\n",
    "\n",
    "def visualize_cam(model, img):\n",
    "    # Extract the last convolutional layer\n",
    "    last_conv_layer = model.get_layer('conv2d_2')\n",
    "    \n",
    "    # Define a new model that outputs the last conv layer and the model's predicted class probabilities\n",
    "    cam_model = tf.keras.Model(model.input, (last_conv_layer.output, model.output))\n",
    "    \n",
    "    # Calculate the gradients of the predicted class with respect to the output feature map of the last conv layer\n",
    "    with tf.GradientTape() as tape:\n",
    "        conv_outputs, predictions = cam_model(img)\n",
    "        predicted_class = tf.argmax(predictions[0])\n",
    "        gradient = tape.gradient(predictions[:, predicted_class], conv_outputs)\n",
    "    \n",
    "    # Compute the weights using global average pooling on the gradients\n",
    "    weights = tf.reduce_mean(gradient[0], axis=(0, 1))\n",
    "    \n",
    "    # Get the feature map values and resize them to match the input image size\n",
    "    feature_map = conv_outputs[0]\n",
    "    cam = np.dot(feature_map, weights)\n",
    "    cam = cv2.resize(cam, (img.shape[2], img.shape[1]))\n",
    "    cam = np.maximum(cam, 0)\n",
    "    \n",
    "    # Normalize the CAM values if the maximum value is non-zero\n",
    "    max_value = np.max(cam)\n",
    "    if max_value != 0:\n",
    "        cam = cam / max_value\n",
    "    \n",
    "    return cam\n",
    "\n",
    "\n",
    "# Choose an image from the test set\n",
    "test_image = X_test[10]\n",
    "\n",
    "# Reshape the image to match the input shape of the model\n",
    "test_image = np.reshape(test_image, (1, 64, 64, 1))\n",
    "\n",
    "# Generate the CAM for the chosen image\n",
    "cam = visualize_cam(best_model, test_image)\n",
    "\n",
    "# Plot the original image and the CAM\n",
    "plt.figure(figsize=(8, 4))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.imshow(test_image[0, :, :, 0], cmap='gray')\n",
    "plt.title('Original Image')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.imshow(test_image[0, :, :, 0], cmap='gray')\n",
    "plt.imshow(cam, cmap='jet', alpha=0.5)\n",
    "plt.title('Class Activation Map')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Class Activation Maps (CAM) of First Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras.backend as K\n",
    "\n",
    "def visualize_cam(model, img):\n",
    "    # Extract the last convolutional layer\n",
    "    last_conv_layer = model.get_layer('conv2d_2')\n",
    "    \n",
    "    # Define a new model that outputs the last conv layer and the model's predicted class probabilities\n",
    "    cam_model = tf.keras.Model(model.input, (last_conv_layer.output, model.output))\n",
    "    \n",
    "    # Reshape the input image to match the expected input shape of the model\n",
    "    img = np.reshape(img, (-1, 64, 64, 1))\n",
    "    \n",
    "    # Calculate the gradients of the predicted class with respect to the output feature map of the last conv layer\n",
    "    with tf.GradientTape() as tape:\n",
    "        conv_outputs, predictions = cam_model(img)\n",
    "        predicted_class = tf.argmax(predictions[0])\n",
    "        gradient = tape.gradient(predictions[:, predicted_class], conv_outputs)\n",
    "    \n",
    "    # Compute the weights using global average pooling on the gradients\n",
    "    weights = tf.reduce_mean(gradient[0], axis=(0, 1))\n",
    "    \n",
    "    # Get the feature map values and resize them to match the input image size\n",
    "    feature_map = conv_outputs[0]\n",
    "    cam = np.dot(feature_map, weights)\n",
    "    cam = cv2.resize(cam, (img.shape[2], img.shape[1]))\n",
    "    cam = np.maximum(cam, 0)\n",
    "    \n",
    "    # Normalize the CAM values if the maximum value is non-zero\n",
    "    max_value = np.max(cam)\n",
    "    if max_value != 0:\n",
    "        cam = cam / max_value\n",
    "    else:\n",
    "        # If the maximum value is zero, set all values to zero\n",
    "        cam = np.zeros_like(cam)\n",
    "    \n",
    "    return cam\n",
    "\n",
    "\n",
    "# Reshape the image to match the expected input shape of the model\n",
    "test_image = np.reshape(test_image, (64, 64))\n",
    "test_image = np.expand_dims(test_image, axis=-1)\n",
    "test_image = np.expand_dims(test_image, axis=0)\n",
    "\n",
    "cam = visualize_cam(best_model, test_image)\n",
    "\n",
    "# Plot the original image and the CAM\n",
    "plt.figure(figsize=(8, 4))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.imshow(test_image[0, :, :, 0], cmap='gray')\n",
    "plt.title('Original Image')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.imshow(test_image[0, :, :, 0], cmap='gray')\n",
    "plt.imshow(cam, cmap='jet', alpha=0.5)\n",
    "plt.title('Class Activation Map')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Class Activation Maps (CAM) of 2nd Layer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow.keras.backend as K\n",
    "\n",
    "def visualize_cam(model, img):\n",
    "    # Extract the last convolutional layer\n",
    "    last_conv_layer = model.get_layer('conv2d_3')\n",
    "    \n",
    "    # Define a new model that outputs the last conv layer and the model's predicted class probabilities\n",
    "    cam_model = tf.keras.Model(model.input, (last_conv_layer.output, model.output))\n",
    "    \n",
    "    # Calculate the gradients of the predicted class with respect to the output feature map of the last conv layer\n",
    "    with tf.GradientTape() as tape:\n",
    "        conv_outputs, predictions = cam_model(img)\n",
    "        predicted_class = tf.argmax(predictions[0])\n",
    "        gradient = tape.gradient(predictions[:, predicted_class], conv_outputs)\n",
    "    \n",
    "    # Compute the weights using global average pooling on the gradients\n",
    "    weights = tf.reduce_mean(gradient[0], axis=(0, 1))\n",
    "    \n",
    "    # Get the feature map values and resize them to match the input image size\n",
    "    feature_map = conv_outputs[0]\n",
    "    cam = np.dot(feature_map, weights)\n",
    "    cam = cv2.resize(cam, (img.shape[2], img.shape[1]))\n",
    "    cam = np.maximum(cam, 0)\n",
    "    cam = cam / np.max(cam)\n",
    "    \n",
    "    return cam\n",
    "\n",
    "\n",
    "# Choose an image from the test set\n",
    "test_image = X_test[3]\n",
    "\n",
    "# Reshape the image to match the input shape of the model\n",
    "test_image = np.reshape(test_image, (1, 64, 64, 1))\n",
    "\n",
    "# Generate the CAM for the chosen image\n",
    "cam = visualize_cam(best_model, test_image)\n",
    "\n",
    "# Plot the original image and the CAM\n",
    "plt.figure(figsize=(8, 4))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.imshow(test_image[0, :, :, 0], cmap='gray')\n",
    "plt.title('Original Image')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.imshow(test_image[0, :, :, 0], cmap='gray')\n",
    "plt.imshow(cam, cmap='jet', alpha=0.5)\n",
    "plt.title('Class Activation Map')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Grad-CAM:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Maps:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_feature_maps(model, img):\n",
    "    # Extract the intermediate feature maps\n",
    "    layer_outputs = [layer.output for layer in model.layers if 'conv2d' in layer.name]\n",
    "    activation_model = tf.keras.Model(inputs=model.input, outputs=layer_outputs)\n",
    "\n",
    "    # Ensure that img is a tensor with consistent shape\n",
    "    img = tf.convert_to_tensor(img, dtype=tf.float32)\n",
    "    img = tf.reshape(img, (1, 64, 64, 1))\n",
    "\n",
    "    # Predict the feature maps\n",
    "    feature_maps = activation_model.predict(img, batch_size=1)\n",
    "\n",
    "    # Plot the feature maps\n",
    "    for layer, feature_map in zip(model.layers, feature_maps):\n",
    "        if 'conv2d' in layer.name:\n",
    "            plt.figure(figsize=(8, 4))\n",
    "            plt.imshow(feature_map[0, :, :, 0], cmap='gray')\n",
    "            plt.title(layer.name + ' Feature Map')\n",
    "            plt.show()\n",
    "\n",
    "# Visualize the feature maps for the chosen image\n",
    "visualize_feature_maps(best_model, test_image)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Filter Visualization:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def visualize_filters(model, layer_name):\n",
    "    layer = model.get_layer(layer_name)\n",
    "    filters, _ = layer.get_weights()\n",
    "\n",
    "    if filters.ndim == 4:\n",
    "        filters = np.moveaxis(filters, -1, 0)\n",
    "\n",
    "    num_filters = filters.shape[0]\n",
    "    num_rows = (num_filters + 7) // 8\n",
    "    plt.figure(figsize=(12, num_rows * 1.5))\n",
    "\n",
    "    for i, filter_ in enumerate(filters):\n",
    "        plt.subplot(num_rows, 8, i+1)\n",
    "        plt.imshow(filter_, cmap='gray')\n",
    "        plt.axis('off')\n",
    "\n",
    "    plt.suptitle(layer_name + ' Filters')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "\n",
    "# Load the best model\n",
    "best_model = tf.keras.models.load_model('best_model.h5')\n",
    "\n",
    "# Visualize the filters of the first convolutional layer\n",
    "visualize_filters(best_model, 'conv2d_2')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Activation Histograms:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_activation_histograms(model, img):\n",
    "    activation_model = tf.keras.Model(inputs=model.input,\n",
    "                                      outputs=[layer.output for layer in model.layers if 'conv2d' in layer.name])\n",
    "    activations = activation_model.predict(img)\n",
    "\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    for i, activation in enumerate(activations):\n",
    "        if 'conv2d_2' in model.layers[i].name:\n",
    "            plt.subplot(2, 4, i+1)\n",
    "            plt.hist(activation.flatten(), bins=50)\n",
    "            plt.title(model.layers[i].name + ' Activation')\n",
    "            plt.xlabel('Activation Value')\n",
    "            plt.ylabel('Frequency')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Plot the activation histograms for the chosen image\n",
    "plot_activation_histograms(best_model, test_image)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Learning Curves:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting the accuracy and loss\n",
    "plt.figure(figsize=(12, 4))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(history.history['accuracy'], label='Train Accuracy')\n",
    "plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(history.history['loss'], label='Training Loss')\n",
    "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameter Optimization:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Architecture Visualization:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Embedding Visualization:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Extract the embeddings from the model\n",
    "embedding_model = tf.keras.Model(inputs=best_model.input,\n",
    "                                 outputs=best_model.get_layer('conv2d_2').output)\n",
    "embeddings = embedding_model.predict(X_test)\n",
    "\n",
    "# Reshape the embeddings array to have two dimensions\n",
    "reshaped_embeddings = embeddings.reshape(embeddings.shape[0], -1)\n",
    "\n",
    "# Standardize the embeddings\n",
    "scaler = StandardScaler()\n",
    "scaled_embeddings = scaler.fit_transform(reshaped_embeddings)\n",
    "\n",
    "# Reduce the dimensionality of the embeddings using t-SNE\n",
    "tsne = TSNE(n_components=2, random_state=42)\n",
    "embeddings_tsne = tsne.fit_transform(scaled_embeddings)\n",
    "\n",
    "# Plot the embeddings\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.scatter(embeddings_tsne[:, 0], embeddings_tsne[:, 1], c=y_true_classes)\n",
    "plt.colorbar()\n",
    "plt.title('Embedding Visualization')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tfgpu",
   "language": "python",
   "name": "tfgpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
