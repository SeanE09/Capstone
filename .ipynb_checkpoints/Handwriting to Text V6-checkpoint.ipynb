{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "conda activate TFgpu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div align=\"center\">\n",
    "  <img src=\"DisplayIMG/Cartoon.png\" alt=\"Image Description\" />\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_cores = os.cpu_count()  # Get number of CPU cores\n",
    "num_cores_to_use = num_cores // 2  # Use half of the cores\n",
    "\n",
    "num_cores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_cores_to_use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import os\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import pandas as pd\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Conv2D, MaxPooling2D, Flatten, Dropout, BatchNormalization\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import glob\n",
    "from sklearn.metrics import classification_report\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from keras_tuner import BayesianOptimization, HyperParameters\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 20 Complete [00h 06m 30s]\n",
      "val_accuracy: 0.5285923779010773\n",
      "\n",
      "Best val_accuracy So Far: 0.6055718660354614\n",
      "Total elapsed time: 02h 03m 30s\n",
      "INFO:tensorflow:Oracle triggered exit\n",
      "Results summary\n",
      "Results in C:\\keras_tuning\\keras_tuner_demo\n",
      "Showing 10 best trials\n",
      "Objective(name=\"val_accuracy\", direction=\"max\")\n",
      "\n",
      "Trial 18 summary\n",
      "Hyperparameters:\n",
      "conv_1_units: 160\n",
      "dropout_1: 0.0\n",
      "conv_2_units: 64\n",
      "dropout_2: 0.2\n",
      "dense_1_units: 800\n",
      "dropout_3: 0.4\n",
      "learning_rate: 0.00022878229268157214\n",
      "Score: 0.6055718660354614\n",
      "\n",
      "Trial 13 summary\n",
      "Hyperparameters:\n",
      "conv_1_units: 256\n",
      "dropout_1: 0.1\n",
      "conv_2_units: 128\n",
      "dropout_2: 0.4\n",
      "dense_1_units: 320\n",
      "dropout_3: 0.2\n",
      "learning_rate: 1.3014694810000588e-05\n",
      "Score: 0.6041055917739868\n",
      "\n",
      "Trial 10 summary\n",
      "Hyperparameters:\n",
      "conv_1_units: 192\n",
      "dropout_1: 0.2\n",
      "conv_2_units: 32\n",
      "dropout_2: 0.0\n",
      "dense_1_units: 896\n",
      "dropout_3: 0.30000000000000004\n",
      "learning_rate: 3.022065504103985e-05\n",
      "Score: 0.5916422307491302\n",
      "\n",
      "Trial 12 summary\n",
      "Hyperparameters:\n",
      "conv_1_units: 96\n",
      "dropout_1: 0.4\n",
      "conv_2_units: 64\n",
      "dropout_2: 0.30000000000000004\n",
      "dense_1_units: 384\n",
      "dropout_3: 0.4\n",
      "learning_rate: 4.768475483649862e-05\n",
      "Score: 0.5791788995265961\n",
      "\n",
      "Trial 16 summary\n",
      "Hyperparameters:\n",
      "conv_1_units: 96\n",
      "dropout_1: 0.30000000000000004\n",
      "conv_2_units: 224\n",
      "dropout_2: 0.1\n",
      "dense_1_units: 992\n",
      "dropout_3: 0.30000000000000004\n",
      "learning_rate: 3.640431988923689e-05\n",
      "Score: 0.5777125954627991\n",
      "\n",
      "Trial 08 summary\n",
      "Hyperparameters:\n",
      "conv_1_units: 256\n",
      "dropout_1: 0.0\n",
      "conv_2_units: 64\n",
      "dropout_2: 0.30000000000000004\n",
      "dense_1_units: 96\n",
      "dropout_3: 0.1\n",
      "learning_rate: 5.147336879284359e-05\n",
      "Score: 0.5762463212013245\n",
      "\n",
      "Trial 17 summary\n",
      "Hyperparameters:\n",
      "conv_1_units: 160\n",
      "dropout_1: 0.1\n",
      "conv_2_units: 224\n",
      "dropout_2: 0.0\n",
      "dense_1_units: 512\n",
      "dropout_3: 0.4\n",
      "learning_rate: 1.1679639634996247e-05\n",
      "Score: 0.5733138024806976\n",
      "\n",
      "Trial 04 summary\n",
      "Hyperparameters:\n",
      "conv_1_units: 224\n",
      "dropout_1: 0.30000000000000004\n",
      "conv_2_units: 32\n",
      "dropout_2: 0.0\n",
      "dense_1_units: 320\n",
      "dropout_3: 0.0\n",
      "learning_rate: 3.1476040230566154e-05\n",
      "Score: 0.5652492642402649\n",
      "\n",
      "Trial 05 summary\n",
      "Hyperparameters:\n",
      "conv_1_units: 64\n",
      "dropout_1: 0.30000000000000004\n",
      "conv_2_units: 96\n",
      "dropout_2: 0.4\n",
      "dense_1_units: 64\n",
      "dropout_3: 0.0\n",
      "learning_rate: 7.927613270599031e-05\n",
      "Score: 0.5549853444099426\n",
      "\n",
      "Trial 19 summary\n",
      "Hyperparameters:\n",
      "conv_1_units: 128\n",
      "dropout_1: 0.30000000000000004\n",
      "conv_2_units: 160\n",
      "dropout_2: 0.1\n",
      "dense_1_units: 512\n",
      "dropout_3: 0.0\n",
      "learning_rate: 1.0910881961820174e-05\n",
      "Score: 0.5285923779010773\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'HyperParameters' object has no attribute 'save'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 111\u001b[0m\n\u001b[0;32m    109\u001b[0m \u001b[38;5;66;03m# Save the best hyperparameters\u001b[39;00m\n\u001b[0;32m    110\u001b[0m best_hyperparameters \u001b[38;5;241m=\u001b[39m tuner\u001b[38;5;241m.\u001b[39mget_best_hyperparameters()[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m--> 111\u001b[0m \u001b[43mbest_hyperparameters\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave\u001b[49m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbest_hyperparameters\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'HyperParameters' object has no attribute 'save'"
     ]
    }
   ],
   "source": [
    "#Run and tune Hyperparameters\n",
    "\n",
    "#----------------------------------------------------------------------\n",
    "\n",
    "import cv2\n",
    "import os\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import pandas as pd\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Conv2D, MaxPooling2D, Flatten, Dropout, BatchNormalization\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import glob\n",
    "from sklearn.metrics import classification_report\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from keras_tuner import BayesianOptimization, HyperParameters\n",
    "import pickle\n",
    "\n",
    "# Get the current directory\n",
    "current_directory = os.getcwd()\n",
    "\n",
    "# Specify the folder name where the images are located\n",
    "folder_name = \"img\"\n",
    "\n",
    "# Construct the folder path\n",
    "folder_path = os.path.join(current_directory, folder_name)\n",
    "\n",
    "# Specify the CSV file path containing image labels\n",
    "csv_file_path = os.path.join(current_directory, \"labels\", \"english-clean.csv\")\n",
    "\n",
    "# Read the CSV file into a DataFrame\n",
    "df = pd.read_csv(csv_file_path)\n",
    "\n",
    "# Create a dictionary where keys are filenames and values are labels\n",
    "labels_dict = df.set_index('image')['label'].to_dict()\n",
    "\n",
    "def load_images_from_folder(folder, labels_dict):\n",
    "    images = []\n",
    "    labels = []\n",
    "    for file_path in glob.glob(os.path.join(folder, \"*.png\")):\n",
    "        img = Image.open(file_path)\n",
    "        if img is not None:\n",
    "            img = img.convert('L')  # Convert image to grayscale\n",
    "            img = img.resize((64, 64))  # Resize the image\n",
    "            np_img = np.array(img)\n",
    "            images.append(np_img)\n",
    "            filename = os.path.basename(file_path)\n",
    "            label = labels_dict[filename]  # Get the label from the filename\n",
    "            labels.append(label)\n",
    "    return images, labels\n",
    "\n",
    "images, labels = load_images_from_folder(folder_path, labels_dict)\n",
    "\n",
    "le = preprocessing.LabelEncoder()\n",
    "numerical_labels = le.fit_transform(labels)  # Convert labels to integers\n",
    "\n",
    "images = np.array(images)  # Convert list of arrays to a single array\n",
    "images = images / 255.0  # Normalize pixel values\n",
    "images = images.reshape(-1, 64, 64, 1)  # Reshape array for CNN\n",
    "\n",
    "encoded_labels = to_categorical(numerical_labels)  # One-hot encode labels\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(images, encoded_labels, test_size=0.2, random_state=42, shuffle=True)\n",
    "\n",
    "def build_model(hp):\n",
    "    model = Sequential()\n",
    "\n",
    "    # First block of convolutions\n",
    "    model.add(Conv2D(hp.Int('conv_1_units', min_value=32, max_value=256, step=32), \n",
    "                     (3, 3), \n",
    "                     activation='relu', \n",
    "                     input_shape=(64, 64, 1)))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(MaxPooling2D((2, 2)))\n",
    "    model.add(Dropout(hp.Float('dropout_1', min_value=0.0, max_value=0.5, step=0.1)))\n",
    "\n",
    "    # Second block of convolutions\n",
    "    model.add(Conv2D(hp.Int('conv_2_units', min_value=32, max_value=256, step=32), \n",
    "                     (3, 3), \n",
    "                     activation='relu'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(MaxPooling2D((2, 2)))\n",
    "    model.add(Dropout(hp.Float('dropout_2', min_value=0.0, max_value=0.5, step=0.1)))\n",
    "\n",
    "    # Fully connected layers\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(hp.Int('dense_1_units', min_value=32, max_value=1024, step=32), activation='relu'))\n",
    "    model.add(Dropout(hp.Float('dropout_3', min_value=0.0, max_value=0.5, step=0.1)))\n",
    "    model.add(Dense(len(le.classes_), activation='softmax'))\n",
    "\n",
    "    # Compile model\n",
    "    model.compile(optimizer=Adam(hp.Float('learning_rate', min_value=1e-5, max_value=1e-2, sampling='LOG')), \n",
    "                  loss='categorical_crossentropy', \n",
    "                  metrics=['accuracy'])\n",
    "\n",
    "    return model\n",
    "\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=5)\n",
    "model_checkpoint = ModelCheckpoint('best_model.h5', monitor='val_loss', save_best_only=True)\n",
    "\n",
    "tuner = BayesianOptimization(\n",
    "    build_model,  # The function to construct the model\n",
    "    objective='val_accuracy',  # The metric to be optimized\n",
    "    max_trials=20,  # The maximum number of iterations for tuning\n",
    "    executions_per_trial=2,  # The number of models that should be built and fit for each trial for robustness purposes\n",
    "    directory=os.path.normpath('C:/keras_tuning'),  # The path to the directory where the search results are stored\n",
    "    project_name='keras_tuner_demo',  # The name of the project. This will be the name of the subdirectory under `directory` where the results are saved\n",
    "    overwrite=True  # Whether or not to overwrite the project if it already exists\n",
    ")\n",
    "\n",
    "tuner.search_space_summary()\n",
    "\n",
    "tuner.search(\n",
    "    X_train,  # Training data\n",
    "    y_train,  # Training labels\n",
    "    epochs=60,  # The number of epochs for training\n",
    "    validation_data=(X_test, y_test),  # Validation data\n",
    "    callbacks=[early_stopping, model_checkpoint]  # Callbacks to be used during training\n",
    ")\n",
    "\n",
    "tuner.results_summary()\n",
    "\n",
    "# Save the best hyperparameters\n",
    "best_hyperparameters = tuner.get_best_hyperparameters()[0]\n",
    "with open('best_hyperparameters.pkl', 'wb') as file:\n",
    "    pickle.dump(best_hyperparameters.values, file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pickle\n",
    "\n",
    "# # Save the best hyperparameters as a dictionary\n",
    "# best_hyperparameters_dict = best_hyperparameters.values\n",
    "\n",
    "# # Specify the file path to save the hyperparameters\n",
    "# hyperparameters_file_path = 'best_hyperparameters.pkl'\n",
    "\n",
    "# # Save the hyperparameters using pickle\n",
    "# with open(hyperparameters_file_path, 'wb') as file:\n",
    "#     pickle.dump(best_hyperparameters_dict, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Run Model off of tuned Hyperparameters\n",
    "\n",
    "#----------------------------------------------------------------\n",
    "\n",
    "import cv2\n",
    "import os\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import pandas as pd\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Conv2D, MaxPooling2D, Flatten, Dropout, BatchNormalization\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import glob\n",
    "from sklearn.metrics import classification_report\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from keras_tuner import HyperParameters\n",
    "\n",
    "# Get the current directory\n",
    "current_directory = os.getcwd()\n",
    "\n",
    "# Specify the folder name where the images are located\n",
    "folder_name = \"img\"\n",
    "\n",
    "# Construct the folder path\n",
    "folder_path = os.path.join(current_directory, folder_name)\n",
    "\n",
    "# Specify the CSV file path containing image labels\n",
    "csv_file_path = os.path.join(current_directory, \"labels\", \"english-clean.csv\")\n",
    "\n",
    "# Read the CSV file into a DataFrame\n",
    "df = pd.read_csv(csv_file_path)\n",
    "\n",
    "# Create a dictionary where keys are filenames and values are labels\n",
    "labels_dict = df.set_index('image')['label'].to_dict()\n",
    "\n",
    "def load_images_from_folder(folder, labels_dict):\n",
    "    images = []\n",
    "    labels = []\n",
    "    for file_path in glob.glob(os.path.join(folder, \"*.png\")):\n",
    "        img = Image.open(file_path)\n",
    "        if img is not None:\n",
    "            img = img.convert('L')  # Convert image to grayscale\n",
    "            img = img.resize((64, 64))  # Resize the image\n",
    "            np_img = np.array(img)\n",
    "            images.append(np_img)\n",
    "            filename = os.path.basename(file_path)\n",
    "            label = labels_dict[filename]  # Get the label from the filename\n",
    "            labels.append(label)\n",
    "    return images, labels\n",
    "\n",
    "images, labels = load_images_from_folder(folder_path, labels_dict)\n",
    "\n",
    "le = preprocessing.LabelEncoder()\n",
    "numerical_labels = le.fit_transform(labels)  # Convert labels to integers\n",
    "\n",
    "images = np.array(images)  # Convert list of arrays to a single array\n",
    "images = images / 255.0  # Normalize pixel values\n",
    "images = images.reshape(-1, 64, 64, 1)  # Reshape array for CNN\n",
    "\n",
    "encoded_labels = to_categorical(numerical_labels)  # One-hot encode labels\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(images, encoded_labels, test_size=0.2, random_state=42, shuffle=True)\n",
    "\n",
    "# Specify the file path to the saved hyperparameters\n",
    "hyperparameters_file_path = 'best_hyperparameters.pkl'\n",
    "\n",
    "# Load the saved hyperparameters\n",
    "with open(hyperparameters_file_path, 'rb') as file:\n",
    "    loaded_hyperparameters_dict = pickle.load(file)\n",
    "\n",
    "# Create a new HyperParameters object and set the loaded hyperparameters\n",
    "loaded_hyperparameters = HyperParameters()\n",
    "loaded_hyperparameters.values = loaded_hyperparameters_dict\n",
    "\n",
    "def build_model(hp):\n",
    "    model = Sequential()\n",
    "\n",
    "    # First block of convolutions\n",
    "    model.add(Conv2D(loaded_hyperparameters.get('conv_1_units'), \n",
    "                     (3, 3), \n",
    "                     activation='relu', \n",
    "                     input_shape=(64, 64, 1)))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(MaxPooling2D((2, 2)))\n",
    "    model.add(Dropout(loaded_hyperparameters.get('dropout_1')))\n",
    "\n",
    "    # Second block of convolutions\n",
    "    model.add(Conv2D(loaded_hyperparameters.get('conv_2_units'), \n",
    "                     (3, 3), \n",
    "                     activation='relu'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(MaxPooling2D((2, 2)))\n",
    "    model.add(Dropout(loaded_hyperparameters.get('dropout_2')))\n",
    "\n",
    "    # Fully connected layers\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(loaded_hyperparameters.get('dense_1_units'), activation='relu'))\n",
    "    model.add(Dropout(loaded_hyperparameters.get('dropout_3')))\n",
    "    model.add(Dense(len(le.classes_), activation='softmax'))\n",
    "\n",
    "    # Compile model\n",
    "    model.compile(optimizer=Adam(loaded_hyperparameters.get('learning_rate')), \n",
    "                  loss='categorical_crossentropy', \n",
    "                  metrics=['accuracy'])\n",
    "\n",
    "    return model\n",
    "\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=3)\n",
    "model_checkpoint = ModelCheckpoint('best_model.h5', monitor='val_loss', save_best_only=True)\n",
    "\n",
    "# Build and train the model with the loaded hyperparameters\n",
    "best_model = build_model(loaded_hyperparameters)\n",
    "history = best_model.fit(X_train, \n",
    "                         y_train, \n",
    "                         epochs=40, \n",
    "                         validation_data=(X_test, y_test), \n",
    "                         callbacks=[early_stopping, model_checkpoint])\n",
    "\n",
    "# Plotting the accuracy and loss\n",
    "plt.figure(figsize=(12, 4))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(history.history['accuracy'], label='Train Accuracy')\n",
    "plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(history.history['loss'], label='Training Loss')\n",
    "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = best_model.predict(X_test)\n",
    "y_pred_classes = np.argmax(y_pred, axis=1)\n",
    "y_true_classes = np.argmax(y_test, axis=1)\n",
    "\n",
    "# Print a classification report\n",
    "print(classification_report(y_true_classes, y_pred_classes, target_names=le.classes_))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Plot training & validation accuracy values\n",
    "plt.figure(figsize=[12,8])\n",
    "plt.plot(history.history['accuracy'])\n",
    "plt.plot(history.history['val_accuracy'])\n",
    "plt.title('Model accuracy')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Test'], loc='upper left')\n",
    "plt.show()\n",
    "\n",
    "# Plot training & validation loss values\n",
    "plt.figure(figsize=[12,8])\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('Model loss')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Test'], loc='upper left')\n",
    "plt.show()\n",
    "\n",
    "# Evaluate the model on the test data using `evaluate`\n",
    "print(\"Evaluate on test data\")\n",
    "results = best_model.evaluate(X_test, y_test, batch_size=128)\n",
    "print(\"test loss, test accuracy:\", results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# #Save Model For Furture Use\n",
    "# model.save('EpochTenThousand')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pickle\n",
    "\n",
    "# # Assume you have a history object from model.fit\n",
    "# # history = model.fit(....)\n",
    "\n",
    "# # Save it under some name\n",
    "# with open('trainHistoryDict', 'wb') as file_pi:\n",
    "#     pickle.dump(history.history, file_pi)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open('trainHistoryDict', 'rb') as file_pi:\n",
    "#     loaded_history = pickle.load(file_pi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model on the training set\n",
    "train_loss, train_accuracy = best_model.evaluate(X_train, y_train)\n",
    "print(\"Train Loss:\", train_loss)\n",
    "print(\"Train Accuracy:\", train_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Evaluate the model on the test set\n",
    "loss, accuracy = best_model.evaluate(X_test, y_test)\n",
    "print(\"Test Loss:\", loss)\n",
    "print(\"Test Accuracy:\", accuracy)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = best_model.predict(X_test)\n",
    "y_pred_labels = np.argmax(y_pred, axis=1)\n",
    "y_true_labels = np.argmax(y_test, axis=1)\n",
    "\n",
    "# Print classification report\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_true_labels, y_pred_labels, zero_division=1))\n",
    "\n",
    "# Print confusion matrix\n",
    "from sklearn.metrics import confusion_matrix\n",
    "confusion_mtx = confusion_matrix(y_true_labels, y_pred_labels)\n",
    "print(\"Confusion Matrix:\")\n",
    "print(confusion_mtx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = best_model.predict(X_test)\n",
    "\n",
    "# Convert probabilities to labels\n",
    "y_pred_labels = np.argmax(y_pred, axis=1)\n",
    "\n",
    "# Also convert the one-hot encoded labels back to label encoding\n",
    "y_true_labels = np.argmax(y_test, axis=1)\n",
    "\n",
    "# Convert numerical labels back to original labels\n",
    "y_pred_labels = le.inverse_transform(y_pred_labels)\n",
    "y_true_labels = le.inverse_transform(y_true_labels)\n",
    "\n",
    "# Calculate confusion matrix\n",
    "confusion_mtx = confusion_matrix(y_true_labels, y_pred_labels)\n",
    "\n",
    "# Create a list of unique labels\n",
    "labels = list(le.classes_)\n",
    "\n",
    "# Set the font scale (this will affect heatmap annotation size)\n",
    "sns.set(font_scale=1.5)\n",
    "\n",
    "# Adjust size of labels, title using rcParams\n",
    "plt.rcParams['xtick.labelsize']=15\n",
    "plt.rcParams['ytick.labelsize']=15\n",
    "\n",
    "# Visualize confusion matrix using seaborn's heatmap\n",
    "plt.figure(figsize=(25, 20))\n",
    "sns.heatmap(confusion_mtx, annot=True, fmt='d', cmap='Blues', xticklabels=labels, yticklabels=labels)\n",
    "plt.xlabel('Predicted Label', fontsize=20)\n",
    "plt.ylabel('True Label', fontsize=20)\n",
    "plt.title('Confusion Matrix', fontsize=25)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tfgpu",
   "language": "python",
   "name": "tfgpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
